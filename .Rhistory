<!--chapter:end:01-intro.Rmd-->
# R Basics
## What is R
## How to install packages
Nowadays there are two ways of installing R packages (that I'm aware of), either using `install.packages`, which is a function shipped with R, or use the devtools R package to install a package from some remote repository other than CRAN, here is a couple of examples:
```r
# This will install the igraph package from CRAN
> install.packages("netdiffuseR")
# This will install the bleeding-edge version from the project's github repo!
> devtools::install_github("USCCANA/netdiffuseR")
```
The first one, using `install.packages`, installs the CRAN version of `netdiffuseR`, whereas the second installs whatever version is plublished on https://github.com/USCCANA/netdiffuseR, which is usually called the development version.
In some cases users may want/need to install packages from command line as some packages need extra configuration to be installed. But we won't need to look at it now.
<!--chapter:end:02-the-basics.Rmd-->
# Week 1: SNS Study
## The Social Network Study
## Reading and _cleaning_ the data
R has several ways of reading data in. You data can be Raw plain files like CSV, tab delimited or specified by column width, for which you can use the [`readr`](https://cran.r-project.org/package=readr) package; or it can be binary files like dta (Stata), Octave, SPSS, for which [`foreign`](https://cran.r-project.org/package=readr) can be used; or it could be excel files in which case you should be using [`readxl`](https://cran.r-project.org/package=readxl). In our case, the data for this session is in Stata13 format, and so we will be using [`readstata13`](https://cran.r-project.org/package=readstata13).
# Chunk 2: 03-read-data
library(dplyr)
library(magrittr)
library(readstata13)
# Reading the data
dat <- read.dta13("SNS datamerged081315edited.dta")
# Taking a look at the file
head(dat[,1:5])
# Chunk 3: 03-filtering-data-pipe
dat_filtered <- select(
dat,
School, photoid,
matches("^sch_friend.+")
)
# Chunk 5: 03-idrange
(photo_id_ran <- range(dat_filtered$photoid))
# Chunk 6: 03-newid
(dat_filtered %<>% mutate(id = School*10000 + photoid)) %>%
head %>%
select(School, photoid, id)
# Chunk 7: 03-loading-tidyr-stringr
library(tidyr)
library(stringr)
# Chunk 8: 03-tibble
dat_filtered <- as_tibble(dat_filtered)
# Chunk 9: 03-reshape
# Maybe too much piping... but its cool!
net <- dat_filtered %>%
select(id, School, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -School) %>%
filter(!is.na(content)) %>%
mutate(
friendid = School*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
# Chunk 1
title: "Applied SNA with R"
author: "George G. Vega Yon"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: USCCANA/appliedsnar
description: "An improvised book on applied Social Network Analysis with R, this is(will be) a compilation of the materials presented in this series of workshop hosted by USC's Center for Applied Network Analysis (CANA)"
---
# Prerequisites
1.  Install R from CRAN: https://www.r-project.org/
2.  (optional) Install Rstudio: https://rstudio.org
While I find RStudio extreamly useful, it is not necesary to use it with R.
<!--chapter:end:index.Rmd-->
# Introduction {#intro}
This book will be build as part of a workshop on Applied Social Network Analysis with R. Its contents will be populated as the sessions take place, and for now there is particular program that we will follow, instead, we have the following workflow:
1.  Participants will share their data and what they need to do with it.
2.  Based on their data, I'll be preparing the sessions trying to show attendees how would I approach the problem, and at the same time, teach by example about the R language.
3.  Materials will be published on this website and, hopefully, video recordings of the sessions.
At least in the first version, the book will be organized by session, this is, one chapter per session.
In general, we will besides of R itself, we will be using R studio and the following R packages: dplyr for data management, stringr for data cleaning, and of course igraph, netdiffuseR (a bit of a bias here), and statnet for our neat network analysis.^[Some of you may be wondering "what about ggplot2 and friends? What about [`tidyverse`](https://www.tidyverse.org/)", well, my short answer is I jumped into R before all of that was that popular. When I started plots were all about [`lattice`](https://CRAN.R-project.org/package=lattice), and after a couple of years on that, about base R graphics. What I'm saying is that so far I have not find a compelling reason to leave my "old-practices" and embrace all the `tidyverse` movement (religion?).]
<!--chapter:end:01-intro.Rmd-->
# R Basics
## What is R
## How to install packages
Nowadays there are two ways of installing R packages (that I'm aware of), either using `install.packages`, which is a function shipped with R, or use the devtools R package to install a package from some remote repository other than CRAN, here is a couple of examples:
```r
# This will install the igraph package from CRAN
> install.packages("netdiffuseR")
# This will install the bleeding-edge version from the project's github repo!
> devtools::install_github("USCCANA/netdiffuseR")
```
The first one, using `install.packages`, installs the CRAN version of `netdiffuseR`, whereas the second installs whatever version is plublished on https://github.com/USCCANA/netdiffuseR, which is usually called the development version.
In some cases users may want/need to install packages from command line as some packages need extra configuration to be installed. But we won't need to look at it now.
<!--chapter:end:02-the-basics.Rmd-->
# Week 1: SNS Study
## The Social Network Study
## Reading and _cleaning_ the data
R has several ways of reading data in. You data can be Raw plain files like CSV, tab delimited or specified by column width, for which you can use the [`readr`](https://cran.r-project.org/package=readr) package; or it can be binary files like dta (Stata), Octave, SPSS, for which [`foreign`](https://cran.r-project.org/package=readr) can be used; or it could be excel files in which case you should be using [`readxl`](https://cran.r-project.org/package=readxl). In our case, the data for this session is in Stata13 format, and so we will be using [`readstata13`](https://cran.r-project.org/package=readstata13).
# Chunk 2: 03-read-data
library(dplyr)
library(magrittr)
library(readstata13)
# Reading the data
dat <- read.dta13("SNS datamerged081315edited.dta")
# Taking a look at the file
head(dat[,1:5])
# Chunk 3: 03-filtering-data-pipe
dat_filtered <- select(
dat,
School, photoid,
matches("^sch_friend.+")
)
# Chunk 5: 03-idrange
(photo_id_ran <- range(dat_filtered$photoid))
# Chunk 6: 03-newid
(dat_filtered %<>% mutate(id = School*10000 + photoid)) %>%
head %>%
select(School, photoid, id)
# Chunk 7: 03-loading-tidyr-stringr
library(tidyr)
library(stringr)
# Chunk 8: 03-tibble
dat_filtered <- as_tibble(dat_filtered)
# Chunk 9: 03-reshape
# Maybe too much piping... but its cool!
net <- dat_filtered %>%
select(id, School, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -School) %>%
filter(!is.na(content)) %>%
mutate(
friendid = School*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
dat_filtered %>%
select(id, School, starts_with("sch_friend"))
dat_filtered %>%
select(id, School, starts_with("sch_friend"))
flush.console()
print(1)
print(1)
print(1)
print(1)
print(1)
print(1)
print(1)
print(1)
print(1)
print(1)
print(1)
flush.console()
flush.console(1)
?flush.console
?flush
flush()
flush.console()
?flush.console
sapply(1:10, function(i) Sys.sleep(1))
invisible(sapply(1:4, function(i) {
Sys.sleep(1)
cat("---", i, "---");
flush.console()
}))
invisible(sapply(1:4, function(i) {
Sys.sleep(1)
cat("---", rep("-",i), "---\n");
flush.console()
}))
?flush.console
flush.console
invisible(sapply(1:4, function(i) {
Sys.sleep(1)
cat("\r---", rep("-",i), "---\n");
flush.console()
}))
while (1) {
cat('\r',format(Sys.time(),'%H:%M:%S'))
flush.console()
}
invisible(sapply(1:4, function(i) {
Sys.sleep(1)
cat('\r', rep("-",i), "---\n");
flush.console()
}))
invisible(sapply(1:4, function(i) {
Sys.sleep(1)
cat('\r', rep("-",i), "---");
flush.console()
}))
Rcpp::evalCpp('Rprintf("\r12)"')
Rcpp::evalCpp('Rprintf("\r12")')
Rcpp::evalCpp('Rprintf("\r%i", 1)')
Rcpp::evalCpp('Rprintf("\\r%i", 1)')
sna::symmetrize
source('~/Documents/appliedsnar/data-raw/03-sns.R', echo=TRUE)
remove.packages("dplyr")
remove.packages("magrittr")
remove.packages("tidyr")
remove.packages("tidyselect")
remove.packages("tibble")
remove.packages("rlang")
install.packages(c("dplyr"))
library(dplyr)
library(magrittr)
library(foreign)
# Reading the data
dat <- foreign::read.dta("03-sns.dta")
# Taking a look at the data's first 5 columns and 5 rows
dat[1:5, 1:10]
View(dat)
edit(dat)
library(tibble)
as_tibble(dat)
dat
as_tibble(dat)
dat2 <- as_tibble(dat)
dat2
# Chunk 1: 03-read-data
library(dplyr)
library(magrittr)
library(foreign)
# Reading the data
dat <- foreign::read.dta("03-sns.dta")
# Taking a look at the data's first 5 columns and 5 rows
dat[1:5, 1:10]
# Chunk 2: 03-idrange
(photo_id_ran <- range(dat$photoid))
# Chunk 3: 03-newid
(dat %<>% mutate(id = school*10000 + photoid)) %>%
head %>%
select(school, photoid, id)
as_tibble(dat)
# Maybe too much piping... but its cool!
net <- dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content)) %>%
mutate(
friendid = school*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
library(tidyr)
library(stringr)
# Maybe too much piping... but its cool!
net <- dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content)) %>%
mutate(
friendid = school*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
net
as_tibble(net)
dat <- as_tibble(dat)
# Maybe too much piping... but its cool!
net <- dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content)) %>%
mutate(
friendid = school*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
net
# Chunk 1: 03-read-data
library(dplyr)
library(magrittr)
library(foreign)
# Reading the data
dat <- foreign::read.dta("03-sns.dta")
# Taking a look at the data's first 5 columns and 5 rows
dat[1:5, 1:10]
# Chunk 2: 03-idrange
(photo_id_ran <- range(dat$photoid))
# Chunk 3: 03-newid
(dat %<>% mutate(id = school*10000 + photoid)) %>%
head %>%
select(school, photoid, id)
# Chunk 4: 03-loading-tidyr-stringr
library(tidyr)
library(stringr)
# Chunk 5: 03-tibble
dat <- as_tibble(dat)
# Chunk 6: 03-tibble-print
dat
# Chunk 7: 03-reshape
# Maybe too much piping... but its cool!
net <- dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content)) %>%
mutate(
friendid = school*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
# Chunk 8
dat %>%
select(id, school, starts_with("sch_friend"))
# Chunk 9
dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school)
# Chunk 10
dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content))
# Chunk 11
dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content)) %>%
mutate(
friendid = school*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
str_extract("sch_friend123", "(?<=[a-z])[0-9]")
str_extract("sch_friend123", "(?<=[a-z][0-9])[0-9]+")
str_extract("sch_friend123", "(?<=friend)[0-9]")
str_extract("sch_friend123", "(?<=friend[0-9])[0-9]+")
str_extract("sch_friend123", "(?<=friend[0-9]).+")
str_extract_all("abc1cde23---7", "[0-9]+")
str_extract_all("abc1cde23---7", "c")
str_extract_all("abc1cde23---7", "[a-z]+")
library(igraph)
# Chunk 1: 03-read-data
library(dplyr)
library(magrittr)
library(foreign)
# Reading the data
dat <- foreign::read.dta("03-sns.dta")
# Taking a look at the data's first 5 columns and 5 rows
dat[1:5, 1:10]
# Chunk 2: 03-idrange
(photo_id_ran <- range(dat$photoid))
# Chunk 3: 03-newid
(dat %<>% mutate(id = school*10000 + photoid)) %>%
head %>%
select(school, photoid, id)
# Chunk 4: 03-loading-tidyr-stringr
library(tidyr)
library(stringr)
# Chunk 5: 03-tibble
dat <- as_tibble(dat)
# Chunk 6: 03-tibble-print
dat
# Chunk 7: 03-reshape
# Maybe too much piping... but its cool!
net <- dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content)) %>%
mutate(
friendid = school*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
# Chunk 8
dat %>%
select(id, school, starts_with("sch_friend"))
# Chunk 9
dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school)
# Chunk 10
dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content))
# Chunk 11
dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content)) %>%
mutate(
friendid = school*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
# Chunk 12: 03-miniregex
str_extract("sch_friend123", "(?<=friend)[0-9]")
str_extract("sch_friend123", "(?<=friend[0-9]).+")
str_extract_all("abc1cde23---7", "[a-z]+")
# Chunk 13: 03-vertex
vertices <- dat %>%
select(id, school, hispanic, female1, starts_with("eversmk"))
library(igraph)
c(1, 2, 3) %in% c(2, 3)
# Chunk 1: 03-read-data
library(dplyr)
library(magrittr)
library(foreign)
# Reading the data
dat <- foreign::read.dta("03-sns.dta")
# Taking a look at the data's first 5 columns and 5 rows
dat[1:5, 1:10]
# Chunk 2: 03-idrange
(photo_id_ran <- range(dat$photoid))
# Chunk 3: 03-newid
(dat %<>% mutate(id = school*10000 + photoid)) %>%
head %>%
select(school, photoid, id)
# Chunk 4: 03-loading-tidyr-stringr
library(tidyr)
library(stringr)
# Chunk 5: 03-tibble
dat <- as_tibble(dat)
# Chunk 6: 03-tibble-print
dat
# Chunk 7: 03-reshape
# Maybe too much piping... but its cool!
net <- dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content)) %>%
mutate(
friendid = school*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
# Chunk 8
dat %>%
select(id, school, starts_with("sch_friend"))
# Chunk 9
dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school)
# Chunk 10
dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content))
# Chunk 11
dat %>%
select(id, school, starts_with("sch_friend")) %>%
gather(key = "varname", value = "content", -id, -school) %>%
filter(!is.na(content)) %>%
mutate(
friendid = school*10000 + content,
year     = str_extract(varname, "(?<=[a-z])[0-9]"),
nnom     = str_extract(varname, "(?<=[a-z][0-9])[0-9]+")
)
# Chunk 12: 03-miniregex
str_extract("sch_friend123", "(?<=friend)[0-9]")
str_extract("sch_friend123", "(?<=friend[0-9]).+")
str_extract_all("abc1cde23---7", "[a-z]+")
# Chunk 13: 03-vertex
vertices <- dat %>%
select(id, school, hispanic, female1, starts_with("eversmk"))
# Chunk 14: 03-igraph
library(igraph)
ig_year1 <- net %>%
filter(year == "1") %>%
select(id, friendid) %>%
graph_from_data_frame(
vertices = vertices
)
# Chunk 15: 03-igraph-bis
ig_year1 <- net %>%
filter(year == "1") %>%
# Extra line, all nominations must be in ego too.
filter(friendid %in% id) %>%
select(id, friendid) %>%
graph_from_data_frame(
vertices = vertices
)
ig_year1
# Chunk 16: 03-listing-attributes
list.vertex.attributes(ig_year1)
list.edge.attributes(ig_year1) # we have no edge attributes here
# Chunk 17: 03-first-10-hispanic
V(ig_year1)$hispanic[1:10]
# Chunk 18: 03-igraph-year1-111
# Which ids are from school 111?
school111ids <- which(V(ig_year1)$school == 111)
# Creating a subgraph
ig_year1_111 <- induced_subgraph(
graph = ig_year1,
vids  = school111ids
)
# Extracting each vectex features as a data.frame
stats <- as_data_frame(ig_year1_111, what = "vertices")
stats
# Computing centrality measures for each vertex
V(ig_year1_111)$indegree   <- degree(ig_year1_111, mode = "in")
V(ig_year1_111)$outdegree  <- degree(ig_year1_111, mode = "out")
V(ig_year1_111)$closeness  <- closeness(ig_year1_111, mode = "total")
V(ig_year1_111)$betweeness <- betweenness(ig_year1_111, normalized = TRUE)
# Extracting each vectex features as a data.frame
stats <- as_data_frame(ig_year1_111, what = "vertices")
str(stats)
str(1)
str(as_data_frame)
quantile(stats$indegree, c(.025, .5, .975))
cbind(
indegree   = quantile(stats$indegree, c(.025, .5, .975)),
outdegree  = quantile(stats$outdegree, c(.025, .5, .975)),
closeness  = quantile(stats$closeness, c(.025, .5, .975)),
betweeness = quantile(stats$betweeness, c(.025, .5, .975))
)
with(stats,
cbind(
indegree   = quantile(indegree, c(.025, .5, .975)),
outdegree  = quantile(outdegree, c(.025, .5, .975)),
closeness  = quantile(closeness, c(.025, .5, .975)),
betweeness = quantile(betweeness, c(.025, .5, .975))
)
)
