[["index.html", "Applied Network Science with R Chapter 1 About", " Applied Network Science with R George G. Vega Yon 2022-04-01 Chapter 1 About 1.0.1 The Book Statistical methods for networked systems are present in most disciplines. Nonetheless, the language differences between disciplines, many methods developed to study specific types of problems can be helpful outside of their original context. This project began as a part of a workshop that took place at USC’s Center for Applied Network Analysis. Now, it is a personal project that I use to gather and study statistical methods to analyze networks, emphasizing social and biological systems. Moreover, the book will use statistical computing methods as a core component when developing these topics. In general, we will, besides R itself, we will be using R studio and the following R packages: dplyr for data management, stringr for data cleaning, and of course igraph, netdiffuseR (a bit of a bias here), and statnet for our neat network analysis.1 You can access the book’s source code at https://github.com/gvegayon/appliedsnar. 1.0.2 The author I am a Research Assistant Professor at the University of Utah’s Division of Epidemiology, where I work on studying Complex Systems using Statistical Computing. I have over ten years of experience developing scientific software focusing on high-performance computing, data visualization, and social network analysis. My training is in Public Policy (M.A. UAI, 2011), Economics (M.Sc. Caltech, 2015), and Biostatistics (Ph.D. USC, 2020). I obtained my Ph.D. in Biostatistics under the supervision of Prof. Paul Marjoram and Prof. Kayla de la Haye, with my dissertation titled “Essays on Bioinformatics and Social Network Analysis: Statistical and Computational Methods for Complex Systems.” If you’d like to learn more about me, please visit my website at https://ggvy.cl. Some of you may be wondering “what about ggplot2 and friends? What about tidyverse”, well, my short answer is I jumped into R before all of that was that popular. When I started, plots were all about lattice, and after a couple of years on that, about base R graphics. What I’m saying is that so far, I have not found a compelling reason to leave my “old practices” and embrace all the tidyverse movement (religion?).↩ "],["introduction.html", "Chapter 2 Introduction", " Chapter 2 Introduction Social Network Analysis and Network Science, have a long scholarly tradition. From social diffusion models to protein-interaction networks, these complex-systems disciplines cover a wide range of problems across scientific fields. Yet, although these could be seen as wildly different, the object under the microscope is the same, networks. With a long history (and insufficient levels of inter-discipline collaboration, if you allow me to say) of scientific advances happing in a somewhat isolated fashion, the potential of cross-pollination between disciplines within network science is immense. This book is an attempt to compile the many methods available in the realm of complexity sciences, provide an in-depth mathematical examination–when possible–, and provide a few examples illustrating their usage. "],["r-basics.html", "Chapter 3 R Basics ", " Chapter 3 R Basics "],["what-is-r.html", "3.1 What is R", " 3.1 What is R A good reference book for both new and advanced user is “The Art of R programming” (Matloff 2011)2 References "],["how-to-install-packages.html", "3.2 How to install packages", " 3.2 How to install packages Nowadays there are two ways of installing R packages (that I’m aware of), either using install.packages, which is a function shipped with R, or use the devtools R package to install a package from some remote repository other than CRAN, here is a couple of examples: # This will install the igraph package from CRAN &gt; install.packages(&quot;netdiffuseR&quot;) # This will install the bleeding-edge version from the project&#39;s github repo! &gt; devtools::install_github(&quot;USCCANA/netdiffuseR&quot;) The first one, using install.packages, installs the CRAN version of netdiffuseR, whereas the second installs whatever version is plublished on https://github.com/USCCANA/netdiffuseR, which is usually called the development version. In some cases users may want/need to install packages from command line as some packages need extra configuration to be installed. But we won’t need to look at it now. "],["prerequisits.html", "3.3 Prerequisits", " 3.3 Prerequisits To install R just follow the instructions available at http://cran.r-project.org RStudio is the most popular Integrated Development Environment (IDE) for R that is developed by the company of the same name. While having RStudio is not a requirement for using netdiffuseR, it is highly recommended. To get RStudio just visit https://www.rstudio.com/products/rstudio/download/. "],["a-gentle-quick-n-dirty-introduction-to-r.html", "3.4 A gentle Quick n’ Dirty Introduction to R", " 3.4 A gentle Quick n’ Dirty Introduction to R Some common tasks in R Getting help (and reading the manual) is THE MOST IMPORTANT thing you should know about. For example, if you want to read the manual (help file) of the read.csv function, you can type either of these: ?read.csv ?&quot;read.csv&quot; help(read.csv) help(&quot;read.csv&quot;) If you are not fully aware of what is the name of the function, you can always use the fuzzy search help.search(&quot;linear regression&quot;) ??&quot;linear regression&quot; In R you can create new objects by either using the assign operator (&lt;-) or the equal sign =, for example, the following 2 are equivalent: a &lt;- 1 a = 1 Historically the assign operator is the most common used. R has several type of objects, the most basic structures in R are vectors, matrix, list, data.frame. Here is an example creating several of these (each line is enclosed with parenthesis so that R prints the resulting element): (a_vector &lt;- 1:9) ## [1] 1 2 3 4 5 6 7 8 9 (another_vect &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9)) ## [1] 1 2 3 4 5 6 7 8 9 (a_string_vec &lt;- c(&quot;I&quot;, &quot;like&quot;, &quot;netdiffuseR&quot;)) ## [1] &quot;I&quot; &quot;like&quot; &quot;netdiffuseR&quot; (a_matrix &lt;- matrix(a_vector, ncol = 3)) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 (a_string_mat &lt;- matrix(letters[1:9], ncol=3)) # Matrices can be of strings too ## [,1] [,2] [,3] ## [1,] &quot;a&quot; &quot;d&quot; &quot;g&quot; ## [2,] &quot;b&quot; &quot;e&quot; &quot;h&quot; ## [3,] &quot;c&quot; &quot;f&quot; &quot;i&quot; (another_mat &lt;- cbind(1:4, 11:14)) # The `cbind` operator does &quot;column bind&quot; ## [,1] [,2] ## [1,] 1 11 ## [2,] 2 12 ## [3,] 3 13 ## [4,] 4 14 (another_mat2 &lt;- rbind(1:4, 11:14)) # The `rbind` operator does &quot;row bind&quot; ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 11 12 13 14 (a_string_mat &lt;- matrix(letters[1:9], ncol = 3)) ## [,1] [,2] [,3] ## [1,] &quot;a&quot; &quot;d&quot; &quot;g&quot; ## [2,] &quot;b&quot; &quot;e&quot; &quot;h&quot; ## [3,] &quot;c&quot; &quot;f&quot; &quot;i&quot; (a_list &lt;- list(a_vector, a_matrix)) ## [[1]] ## [1] 1 2 3 4 5 6 7 8 9 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 (another_list &lt;- list(my_vec = a_vector, my_mat = a_matrix)) # same but with names! ## $my_vec ## [1] 1 2 3 4 5 6 7 8 9 ## ## $my_mat ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 # Data frames can have multiple types of elements, it is a collection of lists (a_data_frame &lt;- data.frame(x = 1:10, y = letters[1:10])) ## x y ## 1 1 a ## 2 2 b ## 3 3 c ## 4 4 d ## 5 5 e ## 6 6 f ## 7 7 g ## 8 8 h ## 9 9 i ## 10 10 j Depending on the type of object, we can access to its components using indexing: a_vector[1:3] # First 3 elements ## [1] 1 2 3 a_string_vec[3] # Third element ## [1] &quot;netdiffuseR&quot; a_matrix[1:2, 1:2] # A sub matrix ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 a_matrix[,3] # Third column ## [1] 7 8 9 a_matrix[3,] # Third row ## [1] 3 6 9 a_string_mat[1:6] # First 6 elements of the matrix. R stores matrices by column. ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; # These three are equivalent another_list[[1]] ## [1] 1 2 3 4 5 6 7 8 9 another_list$my_vec ## [1] 1 2 3 4 5 6 7 8 9 another_list[[&quot;my_vec&quot;]] ## [1] 1 2 3 4 5 6 7 8 9 # Data frames are just like lists a_data_frame[[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 a_data_frame[,1] ## [1] 1 2 3 4 5 6 7 8 9 10 a_data_frame[[&quot;x&quot;]] ## [1] 1 2 3 4 5 6 7 8 9 10 a_data_frame$x ## [1] 1 2 3 4 5 6 7 8 9 10 Control-flow statements # The oldfashion forloop for (i in 1:10) { print(paste(&quot;I&#39;m step&quot;, i, &quot;/&quot;, 10)) } ## [1] &quot;I&#39;m step 1 / 10&quot; ## [1] &quot;I&#39;m step 2 / 10&quot; ## [1] &quot;I&#39;m step 3 / 10&quot; ## [1] &quot;I&#39;m step 4 / 10&quot; ## [1] &quot;I&#39;m step 5 / 10&quot; ## [1] &quot;I&#39;m step 6 / 10&quot; ## [1] &quot;I&#39;m step 7 / 10&quot; ## [1] &quot;I&#39;m step 8 / 10&quot; ## [1] &quot;I&#39;m step 9 / 10&quot; ## [1] &quot;I&#39;m step 10 / 10&quot; # A nice ifelse for (i in 1:10) { if (i %% 2) # Modulus operand print(paste(&quot;I&#39;m step&quot;, i, &quot;/&quot;, 10, &quot;(and I&#39;m odd)&quot;)) else print(paste(&quot;I&#39;m step&quot;, i, &quot;/&quot;, 10, &quot;(and I&#39;m even)&quot;)) } ## [1] &quot;I&#39;m step 1 / 10 (and I&#39;m odd)&quot; ## [1] &quot;I&#39;m step 2 / 10 (and I&#39;m even)&quot; ## [1] &quot;I&#39;m step 3 / 10 (and I&#39;m odd)&quot; ## [1] &quot;I&#39;m step 4 / 10 (and I&#39;m even)&quot; ## [1] &quot;I&#39;m step 5 / 10 (and I&#39;m odd)&quot; ## [1] &quot;I&#39;m step 6 / 10 (and I&#39;m even)&quot; ## [1] &quot;I&#39;m step 7 / 10 (and I&#39;m odd)&quot; ## [1] &quot;I&#39;m step 8 / 10 (and I&#39;m even)&quot; ## [1] &quot;I&#39;m step 9 / 10 (and I&#39;m odd)&quot; ## [1] &quot;I&#39;m step 10 / 10 (and I&#39;m even)&quot; # A while i &lt;- 10 while (i &gt; 0) { print(paste(&quot;I&#39;m step&quot;, i, &quot;/&quot;, 10)) i &lt;- i - 1 } ## [1] &quot;I&#39;m step 10 / 10&quot; ## [1] &quot;I&#39;m step 9 / 10&quot; ## [1] &quot;I&#39;m step 8 / 10&quot; ## [1] &quot;I&#39;m step 7 / 10&quot; ## [1] &quot;I&#39;m step 6 / 10&quot; ## [1] &quot;I&#39;m step 5 / 10&quot; ## [1] &quot;I&#39;m step 4 / 10&quot; ## [1] &quot;I&#39;m step 3 / 10&quot; ## [1] &quot;I&#39;m step 2 / 10&quot; ## [1] &quot;I&#39;m step 1 / 10&quot; R has a very nice set of pseudo random number generation functions. In general, distribution functions have the following name structure: Random Number Generation: r[name-of-the-distribution], e.g. rnorm for normal, runif for uniform. Density function: d[name-of-the-distribution], e.g. dnorm for normal, dunif for uniform. Cumulative Distribution Function (CDF): p[name-of-the-distribution], e.g. pnorm for normal, punif for uniform. Inverse (quantile) function: q[name-of-the-distribution], e.g. qnorm for the normal, qunif for the uniform. Here are some examples: # To ensure reproducibility set.seed(1231) # 100,000 Unif(0,1) numbers x &lt;- runif(1e5) hist(x) # 100,000 N(0,1) numbers x &lt;- rnorm(1e5) hist(x) # 100,000 N(10,25) numbers x &lt;- rnorm(1e5, mean = 10, sd = 5) hist(x) # 100,000 Poisson(5) numbers x &lt;- rpois(1e5, lambda = 5) hist(x) # 100,000 rexp(5) numbers x &lt;- rexp(1e5, 5) hist(x) More distributions available at ??Distributions. For a nice intro to R, take a look at “The Art of R Programming” by Norman Matloff. For more advanced users, take a look at “Advanced R” by Hadley Wickham. For this book, we need the following R Core Team (2017b) Install R from CRAN: https://www.r-project.org/ (optional) Install Rstudio: https://rstudio.org While I find RStudio extremely useful, it is not necessary to use it with R. References "],["network-nomination-data.html", "Chapter 4 Network Nomination Data", " Chapter 4 Network Nomination Data You can download the data for this chapter here. The codebook for the data provided here is in the appendix. The goals for this chapter are: Read the data into R, Create a network with it, Compute descriptive statistics Visualize the network "],["data-preprocessing.html", "4.1 Data preprocessing", " 4.1 Data preprocessing 4.1.1 Reading the data into R R has several ways of reading data. Your data can be Raw plain files like CSV, tab-delimited, or specified by column width. To read plain-text data, you can use the readr package (Wickham, Hester, and Francois 2017). In the case of binary files, like Stata, Octave, or SPSS files, you can use the R package foreign (R Core Team 2017a). If your data is formatted as Microsoft spreadsheets, the readxl R package (Wickham and Bryan 2017) is the alternative to use. In our case, the data for this session is in Stata format: library(foreign) # Reading the data dat &lt;- foreign::read.dta(&quot;03-sns.dta&quot;) # Taking a look at the data&#39;s first 5 columns and 5 rows dat[1:5, 1:10] ## photoid school hispanic female1 female2 female3 female4 grades1 grades2 ## 1 1 111 1 NA NA 0 0 NA NA ## 2 2 111 1 0 NA NA 0 3.0 NA ## 3 7 111 0 1 1 1 1 5.0 4.5 ## 4 13 111 1 1 1 1 1 2.5 2.5 ## 5 14 111 1 1 1 1 NA 3.0 3.5 ## grades3 ## 1 3.5 ## 2 NA ## 3 4.0 ## 4 2.5 ## 5 3.5 4.1.2 Creating a unique id for each participant Now suppose that we want to create a unique id using the school and photo id. In this case, since both variables are numeric, a good way of doing it is to encode the id. For example, the last three numbers are the photoid and the first ones are the school id. To do this, we need to take into account the range of the variables: (photo_id_ran &lt;- range(dat$photoid)) ## [1] 1 2074 As the variable spans up to 2074, we need to set the last 4 units of the variable to store the photoid. We will use dplyr (Wickham et al. 2017) and magrittr (Bache and Wickham 2014)] (the pipe operator, %&gt;%) to create this variable, and we will call it… id (mind blowing, right?): library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(magrittr) (dat %&lt;&gt;% mutate(id = school*10000 + photoid)) %&gt;% head %&gt;% select(school, photoid, id) ## school photoid id ## 1 111 1 1110001 ## 2 111 2 1110002 ## 3 111 7 1110007 ## 4 111 13 1110013 ## 5 111 14 1110014 ## 6 111 15 1110015 Wow, what happened in the last three lines of code! What is that %&gt;%? Well, that’s the pipe operator, and it is an appealing way of writing nested function calls. In this case, instead of writing something like: dat_filtered$id &lt;- dat_filtered$school*10000 + dat_filtered$photoid subset(head(dat_filtered), select = c(school, photoid, id)) References "],["creating-a-network.html", "4.2 Creating a network", " 4.2 Creating a network We want to build a social network. For that, we either use an adjacency matrix or an edgelist. Each individual of the SNS data nominated 19 friends from school. We will use those nominations to create the social network. In this case, we will create the network by coercing the dataset into an edgelist. 4.2.1 From survey to edgelist Let’s start by loading a couple of handy R packages. We will load tidyr (Wickham and Henry 2017) and stringr (Wickham 2017). We will use the first, tidyr, to reshape the data. The second, stringr, will help us processing strings using regular expressions3. library(tidyr) library(stringr) Optionally, we can use the tibble type of object, which is an alternative to the actual data.frame. This object is said to provide more efficient methods for matrices and data frames. dat &lt;- as_tibble(dat) What I like from tibbles is that when you print them on the console, these actually look nice: dat ## # A tibble: 2,164 × 100 ## photoid school hispanic female1 female2 female3 female4 grades1 grades2 ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 111 1 NA NA 0 0 NA NA ## 2 2 111 1 0 NA NA 0 3 NA ## 3 7 111 0 1 1 1 1 5 4.5 ## 4 13 111 1 1 1 1 1 2.5 2.5 ## 5 14 111 1 1 1 1 NA 3 3.5 ## 6 15 111 1 0 0 0 0 2.5 2.5 ## 7 20 111 1 1 1 1 1 2.5 2.5 ## 8 22 111 1 NA NA 0 0 NA NA ## 9 25 111 0 1 1 NA 1 4.5 3.5 ## 10 27 111 1 0 NA 0 0 3.5 NA ## # … with 2,154 more rows, and 91 more variables: grades3 &lt;dbl&gt;, grades4 &lt;dbl&gt;, ## # eversmk1 &lt;int&gt;, eversmk2 &lt;int&gt;, eversmk3 &lt;int&gt;, eversmk4 &lt;int&gt;, ## # everdrk1 &lt;int&gt;, everdrk2 &lt;int&gt;, everdrk3 &lt;int&gt;, everdrk4 &lt;int&gt;, ## # home1 &lt;int&gt;, home2 &lt;int&gt;, home3 &lt;int&gt;, home4 &lt;int&gt;, sch_friend11 &lt;int&gt;, ## # sch_friend12 &lt;int&gt;, sch_friend13 &lt;int&gt;, sch_friend14 &lt;int&gt;, ## # sch_friend15 &lt;int&gt;, sch_friend16 &lt;int&gt;, sch_friend17 &lt;int&gt;, ## # sch_friend18 &lt;int&gt;, sch_friend19 &lt;int&gt;, sch_friend110 &lt;int&gt;, … # Maybe too much piping... but its cool! net &lt;- dat %&gt;% select(id, school, starts_with(&quot;sch_friend&quot;)) %&gt;% gather(key = &quot;varname&quot;, value = &quot;content&quot;, -id, -school) %&gt;% filter(!is.na(content)) %&gt;% mutate( friendid = school*10000 + content, year = as.integer(str_extract(varname, &quot;(?&lt;=[a-z])[0-9]&quot;)), nnom = as.integer(str_extract(varname, &quot;(?&lt;=[a-z][0-9])[0-9]+&quot;)) ) Let’s take a look at this step by step: First, we subset the data: We want to keep id, school, sch_friend*. For the later, we use the function starts_with (from the tidyselect package). The latter allows us to select all variables that start with the word “sch_friend”, which means that sch_friend11, sch_friend12, ... will be selected. dat %&gt;% select(id, school, starts_with(&quot;sch_friend&quot;)) ## # A tibble: 2,164 × 78 ## id school sch_friend11 sch_friend12 sch_friend13 sch_friend14 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1110001 111 NA NA NA NA ## 2 1110002 111 424 423 426 289 ## 3 1110007 111 629 505 NA NA ## 4 1110013 111 232 569 NA NA ## 5 1110014 111 582 134 41 592 ## 6 1110015 111 26 488 81 138 ## 7 1110020 111 528 NA 492 395 ## 8 1110022 111 NA NA NA NA ## 9 1110025 111 135 185 553 84 ## 10 1110027 111 346 168 559 5 ## # … with 2,154 more rows, and 72 more variables: sch_friend15 &lt;int&gt;, ## # sch_friend16 &lt;int&gt;, sch_friend17 &lt;int&gt;, sch_friend18 &lt;int&gt;, ## # sch_friend19 &lt;int&gt;, sch_friend110 &lt;int&gt;, sch_friend111 &lt;int&gt;, ## # sch_friend112 &lt;int&gt;, sch_friend113 &lt;int&gt;, sch_friend114 &lt;int&gt;, ## # sch_friend115 &lt;int&gt;, sch_friend116 &lt;int&gt;, sch_friend117 &lt;int&gt;, ## # sch_friend118 &lt;int&gt;, sch_friend119 &lt;int&gt;, sch_friend21 &lt;int&gt;, ## # sch_friend22 &lt;int&gt;, sch_friend23 &lt;int&gt;, sch_friend24 &lt;int&gt;, … Then, we reshape it to long format: By transposing all the sch_friend* to long format. We do this using the function gather (from the tidyr package); an alternative to the reshape function, which I find easier to use. Let’s see how it works: dat %&gt;% select(id, school, starts_with(&quot;sch_friend&quot;)) %&gt;% gather(key = &quot;varname&quot;, value = &quot;content&quot;, -id, -school) ## # A tibble: 164,464 × 4 ## id school varname content ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1110001 111 sch_friend11 NA ## 2 1110002 111 sch_friend11 424 ## 3 1110007 111 sch_friend11 629 ## 4 1110013 111 sch_friend11 232 ## 5 1110014 111 sch_friend11 582 ## 6 1110015 111 sch_friend11 26 ## 7 1110020 111 sch_friend11 528 ## 8 1110022 111 sch_friend11 NA ## 9 1110025 111 sch_friend11 135 ## 10 1110027 111 sch_friend11 346 ## # … with 164,454 more rows In this case, the key parameter sets the name of the variable that will contain the name of the variable that was reshaped, while value is the name of the variable that will hold the content of the data (that’s why I named those like that). The -id, -school bit tells the function to “drop” those variables before reshaping. In other words, “reshape everything but id and school.” Also, notice that we passed from 2164 rows to 19 (nominations) * 2164 (subjects) * 4 (waves) = 164464 rows, as expected. As the nomination data can be empty for some cells, we need to take care of those cases, the NAs, so we filter the data: dat %&gt;% select(id, school, starts_with(&quot;sch_friend&quot;)) %&gt;% gather(key = &quot;varname&quot;, value = &quot;content&quot;, -id, -school) %&gt;% filter(!is.na(content)) ## # A tibble: 39,561 × 4 ## id school varname content ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1110002 111 sch_friend11 424 ## 2 1110007 111 sch_friend11 629 ## 3 1110013 111 sch_friend11 232 ## 4 1110014 111 sch_friend11 582 ## 5 1110015 111 sch_friend11 26 ## 6 1110020 111 sch_friend11 528 ## 7 1110025 111 sch_friend11 135 ## 8 1110027 111 sch_friend11 346 ## 9 1110029 111 sch_friend11 369 ## 10 1110030 111 sch_friend11 462 ## # … with 39,551 more rows And finally, we create three new variables from this dataset: friendid,, year, and nom_num (nomination number). All using regular expressions: dat %&gt;% select(id, school, starts_with(&quot;sch_friend&quot;)) %&gt;% gather(key = &quot;varname&quot;, value = &quot;content&quot;, -id, -school) %&gt;% filter(!is.na(content)) %&gt;% mutate( friendid = school*10000 + content, year = as.integer(str_extract(varname, &quot;(?&lt;=[a-z])[0-9]&quot;)), nnom = as.integer(str_extract(varname, &quot;(?&lt;=[a-z][0-9])[0-9]+&quot;)) ) ## # A tibble: 39,561 × 7 ## id school varname content friendid year nnom ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1110002 111 sch_friend11 424 1110424 1 1 ## 2 1110007 111 sch_friend11 629 1110629 1 1 ## 3 1110013 111 sch_friend11 232 1110232 1 1 ## 4 1110014 111 sch_friend11 582 1110582 1 1 ## 5 1110015 111 sch_friend11 26 1110026 1 1 ## 6 1110020 111 sch_friend11 528 1110528 1 1 ## 7 1110025 111 sch_friend11 135 1110135 1 1 ## 8 1110027 111 sch_friend11 346 1110346 1 1 ## 9 1110029 111 sch_friend11 369 1110369 1 1 ## 10 1110030 111 sch_friend11 462 1110462 1 1 ## # … with 39,551 more rows The regular expression (?&lt;=[a-z]) matches a string preceded by any letter from a to z. In contrast, the expression [0-9] matches a single number. Hence, from the string \"sch_friend12\", the regular expression will only match the 1, as it is the only number followed by a letter. The expression (?&lt;=[a-z][0-9]) matches a string preceded by a lower case letter and a one-digit number. Finally, the expression [0-9]+ matches a string of numbers–so it could be more than one. Hence, from the string \"sch_friend12\", we will get 2: str_extract(&quot;sch_friend12&quot;, &quot;(?&lt;=[a-z])[0-9]&quot;) ## [1] &quot;1&quot; str_extract(&quot;sch_friend12&quot;, &quot;(?&lt;=[a-z][0-9])[0-9]+&quot;) ## [1] &quot;2&quot; And finally, the as.integer function coerces the returning value from the str_extract function from character to integer. Now that we have this edgelist, we can create an igraph object 4.2.2 igraph network For coercing the edgelist into an igraph object, we will be using the graph_from_data_frame function in igraph (Csardi and Nepusz 2006). This function receives the following arguments: a data frame where the two first columns are “source” (ego) and “target” (alter), an indicator of whether the network is directed or not, and an optional data frame with vertices, in which’s first column should contain the vertex ids. Using the optional vertices argument is a good practice since, by doing so, you are telling the function what ids that you are expecting to find. Using the original dataset, we will create a data frame name vertices: vertex_attrs &lt;- dat %&gt;% select(id, school, hispanic, female1, starts_with(&quot;eversmk&quot;)) Now, let’s now use the function graph_from_data_frame to create an igraph object: library(igraph) ig_year1 &lt;- net %&gt;% filter(year == &quot;1&quot;) %&gt;% select(id, friendid, nnom) %&gt;% graph_from_data_frame( vertices = vertex_attrs ) ## Error in graph_from_data_frame(., vertices = vertex_attrs): Some vertex names in edge list are not listed in vertex data frame Ups! It seems that individuals are making nominations to other students not included in the survey. How to solve that? Well, it all depends on what you need to do! In this case, we will go for the quietly-remove-em’-and-don’t-tell strategy: ig_year1 &lt;- net %&gt;% filter(year == &quot;1&quot;) %&gt;% # Extra line, all nominations must be in ego too. filter(friendid %in% id) %&gt;% select(id, friendid, nnom) %&gt;% graph_from_data_frame( vertices = vertex_attrs ) ig_year1 ## IGRAPH bdeb582 DN-- 2164 9514 -- ## + attr: name (v/c), school (v/n), hispanic (v/n), female1 (v/n), ## | eversmk1 (v/n), eversmk2 (v/n), eversmk3 (v/n), eversmk4 (v/n), nnom ## | (e/n) ## + edges from bdeb582 (vertex names): ## [1] 1110007-&gt;1110629 1110013-&gt;1110232 1110014-&gt;1110582 1110015-&gt;1110026 ## [5] 1110025-&gt;1110135 1110027-&gt;1110346 1110029-&gt;1110369 1110035-&gt;1110034 ## [9] 1110040-&gt;1110390 1110041-&gt;1110557 1110044-&gt;1110027 1110046-&gt;1110030 ## [13] 1110050-&gt;1110086 1110057-&gt;1110263 1110069-&gt;1110544 1110071-&gt;1110167 ## [17] 1110072-&gt;1110289 1110073-&gt;1110014 1110075-&gt;1110352 1110084-&gt;1110305 ## [21] 1110086-&gt;1110206 1110093-&gt;1110040 1110094-&gt;1110483 1110095-&gt;1110043 ## + ... omitted several edges So there we have our network with 2164 nodes and 9514 edges. The following steps: get some descriptive stats and visualize our network. References "],["network-descriptive-stats.html", "4.3 Network descriptive stats", " 4.3 Network descriptive stats While we could do all networks at once, in this part, we will focus on computing some network statistics for one of the schools only. We start by school 111. The first question that you should be asking yourself now is, “how can I get that information from the igraph object?.” Vertex and edges attributes can be accessed via the V and E functions, respectively; moreover, we can list what vertex/edge attributes are available: list.vertex.attributes(ig_year1) ## [1] &quot;name&quot; &quot;school&quot; &quot;hispanic&quot; &quot;female1&quot; &quot;eversmk1&quot; &quot;eversmk2&quot; &quot;eversmk3&quot; ## [8] &quot;eversmk4&quot; list.edge.attributes(ig_year1) ## [1] &quot;nnom&quot; Just like we would do with data frames, accessing vertex attributes is done via the dollar sign operator $. Together with the V function; for example, accessing the first ten elements of the variable hispanic can be done as follows: V(ig_year1)$hispanic[1:10] ## [1] 1 1 0 1 1 1 1 1 0 1 Now that you know how to access vertex attributes, we can get the network corresponding to school 111 by identifying which vertices are part of it and pass that information to the induced_subgraph function: # Which ids are from school 111? school111ids &lt;- which(V(ig_year1)$school == 111) # Creating a subgraph ig_year1_111 &lt;- induced_subgraph( graph = ig_year1, vids = school111ids ) The which function in R returns a vector of indices indicating which elements pass the test, returning true and false, otherwise. In our case, it will result in a vector of indices of the vertices which have the attribute school equal to 111. With the subgraph, we can compute different centrality measures4 for each vertex and store them in the igraph object itself: # Computing centrality measures for each vertex V(ig_year1_111)$indegree &lt;- degree(ig_year1_111, mode = &quot;in&quot;) V(ig_year1_111)$outdegree &lt;- degree(ig_year1_111, mode = &quot;out&quot;) V(ig_year1_111)$closeness &lt;- closeness(ig_year1_111, mode = &quot;total&quot;) ## Warning in closeness(ig_year1_111, mode = &quot;total&quot;): At ## centrality.c:2874 :closeness centrality is not well-defined for disconnected ## graphs V(ig_year1_111)$betweeness &lt;- betweenness(ig_year1_111, normalized = TRUE) From here, we can go back to our old habits and get the set of vertex attributes as a data frame so we can compute some summary statistics on the centrality measurements that we just got # Extracting each vectex features as a data.frame stats &lt;- as_data_frame(ig_year1_111, what = &quot;vertices&quot;) # Computing quantiles for each variable stats_degree &lt;- with(stats, { cbind( indegree = quantile(indegree, c(.025, .5, .975)), outdegree = quantile(outdegree, c(.025, .5, .975)), closeness = quantile(closeness, c(.025, .5, .975)), betweeness = quantile(betweeness, c(.025, .5, .975)) ) }) stats_degree ## indegree outdegree closeness betweeness ## 2.5% 0 0 3.526640e-06 0.000000000 ## 50% 4 4 1.595431e-05 0.001879006 ## 97.5% 16 16 1.601822e-05 0.016591048 The with function is somewhat similar to what dplyr allows us to do when we want to work with the dataset but without mentioning its name everytime that we ask for a variable. Without using the with function, the previous could have been done as follows: stats_degree &lt;- cbind( indegree = quantile(stats$indegree, c(.025, .5, .975)), outdegree = quantile(stats$outdegree, c(.025, .5, .975)), closeness = quantile(stats$closeness, c(.025, .5, .975)), betweeness = quantile(stats$betweeness, c(.025, .5, .975)) ) Now we will compute some statistics at the graph level: cbind( size = vcount(ig_year1_111), nedges = ecount(ig_year1_111), density = edge_density(ig_year1_111), recip = reciprocity(ig_year1_111), centr = centr_betw(ig_year1_111)$centralization, pathLen = mean_distance(ig_year1_111) ) ## size nedges density recip centr pathLen ## [1,] 533 2638 0.009303277 0.3731513 0.02179154 4.23678 Triadic census triadic &lt;- triad_census(ig_year1_111) triadic ## [1] 24059676 724389 290849 3619 3383 4401 3219 2997 ## [9] 407 33 836 235 163 137 277 85 To get a nicer view of this, we can use a table that I retrieved from ?triad_census. Moreover, we can normalize the triadic object by its sum instead of looking at raw counts. That way, we get proportions instead5 knitr::kable(cbind( Pcent = triadic/sum(triadic)*100, read.csv(&quot;triadic_census.csv&quot;) ), digits = 2) Pcent code description 95.88 003 A,B,C, the empty graph. 2.89 012 A-&gt;B, C, the graph with a single directed edge. 1.16 102 A&lt;-&gt;B, C, the graph with a mutual connection between two vertices. 0.01 021D A&lt;-B-&gt;C, the out-star. 0.01 021U A-&gt;B&lt;-C, the in-star. 0.02 021C A-&gt;B-&gt;C, directed line. 0.01 111D A&lt;-&gt;B&lt;-C. 0.01 111U A&lt;-&gt;B-&gt;C. 0.00 030T A-&gt;B&lt;-C, A-&gt;C. 0.00 030C A&lt;-B&lt;-C, A-&gt;C. 0.00 201 A&lt;-&gt;B&lt;-&gt;C. 0.00 120D A&lt;-B-&gt;C, A&lt;-&gt;C. 0.00 120U A-&gt;B&lt;-C, A&lt;-&gt;C. 0.00 120C A-&gt;B-&gt;C, A&lt;-&gt;C. 0.00 210 A-&gt;B&lt;-&gt;C, A&lt;-&gt;C. 0.00 300 A&lt;-&gt;B&lt;-&gt;C, A&lt;-&gt;C, the complete graph. For more information about the different centrality measurements, please take a look at the “Centrality” article on Wikipedia.↩ During our workshop, Prof. De la Haye suggested using \\({n \\choose 3}\\) as a normalizing constant. It turns out that sum(triadic) = choose(n, 3)! So either approach is correct.↩ "],["plotting-the-network-in-igraph.html", "4.4 Plotting the network in igraph", " 4.4 Plotting the network in igraph 4.4.1 Single plot Let’s take a look at how does our network looks like when we use the default parameters in the plot method of the igraph object: plot(ig_year1) Figure 4.1: A not very nice network plot. This is what we get with the default parameters in igraph. Not very nice, right? A couple of things with this plot: We are looking at all schools simultaneously, which does not make sense. So, instead of plotting ig_year1, we will focus on ig_year1_111. All the vertices have the same size and are overlapping. Instead of using the default size, we will size the vertices by indegree using the degree function and passing the vector of degrees to vertex.size.6 Given the number of vertices in these networks, the labels are not useful here. So we will remove them by setting vertex.label = NA. Moreover, we will reduce the size of the arrows’ tip by setting edge.arrow.size = 0.25. And finally, we will set the color of each vertex to be a function of whether the individual is Hispanic or not. For this last bit we need to go a bit more of programming: col_hispanic &lt;- V(ig_year1_111)$hispanic + 1 col_hispanic &lt;- coalesce(col_hispanic, 3) col_hispanic &lt;- c(&quot;steelblue&quot;, &quot;tomato&quot;, &quot;white&quot;)[col_hispanic] Line by line, we did the following: The first line added one to all no NA values so that the 0s (non-Hispanic) turned to 1s and the 1s (Hispanic) turned to 2s. The second line replaced all NAs with the number three so that our vector col_hispanic now ranges from one to three with no NAs in it. In the last line, we created a vector of colors. Essentially, what we are doing here is telling R to create a vector of length length(col_hispanic) by selecting elements by index from the vector c(\"steelblue\", \"tomato\", \"white\"). This way, if, for example, the first element of the vector col_hispanic was a 3, our new vector of colors would have a \"white\" in it. To make sure we know we are right, let’s print the first 10 elements of our new vector of colors together with the original hispanic column: cbind( original = V(ig_year1_111)$hispanic[1:10], colors = col_hispanic[1:10] ) ## original colors ## [1,] &quot;1&quot; &quot;tomato&quot; ## [2,] &quot;1&quot; &quot;tomato&quot; ## [3,] &quot;0&quot; &quot;steelblue&quot; ## [4,] &quot;1&quot; &quot;tomato&quot; ## [5,] &quot;1&quot; &quot;tomato&quot; ## [6,] &quot;1&quot; &quot;tomato&quot; ## [7,] &quot;1&quot; &quot;tomato&quot; ## [8,] &quot;1&quot; &quot;tomato&quot; ## [9,] &quot;0&quot; &quot;steelblue&quot; ## [10,] &quot;1&quot; &quot;tomato&quot; With our nice vector of colors, now we can pass it to plot.igraph (which we call implicitly by just calling plot), via the vertex.color argument: # Fancy graph set.seed(1) plot( ig_year1_111, vertex.size = degree(ig_year1_111)/10 +1, vertex.label = NA, edge.arrow.size = .25, vertex.color = col_hispanic ) Figure 4.2: Friends network in time 1 for school 111. Nice! So it does look better. The only problem is that we have a lot of isolates. Let’s try again by drawing the same plot without isolates. To do so, we need to filter the graph, for which we will use the function induced_subgraph # Which vertices are not isolates? which_ids &lt;- which(degree(ig_year1_111, mode = &quot;total&quot;) &gt; 0) # Getting the subgraph ig_year1_111_sub &lt;- induced_subgraph(ig_year1_111, which_ids) # We need to get the same subset in col_hispanic col_hispanic &lt;- col_hispanic[which_ids] # Fancy graph set.seed(1) plot( ig_year1_111_sub, vertex.size = degree(ig_year1_111_sub)/5 +1, vertex.label = NA, edge.arrow.size = .25, vertex.color = col_hispanic ) Figure 4.3: Friends network in time 1 for school 111. The graph excludes isolates. Now that’s better! An interesting pattern that shows up is that individuals seem to cluster by whether they are Hispanic or not. We can write this as a function to avoid copying and pasting the code \\(n\\) times (supposing that we want to create a plot similar to this \\(n\\) times). We do the latter in the following subsection. 4.4.2 Multiple plots When you are repeating yourself repeatedly, it is a good idea to write down a sequence of commands as a function. In this case, since we will be running the same type of plot for all schools/waves, we write a function in which the only things that change are: (a) the school id, and (b) the color of the nodes. myplot &lt;- function( net, schoolid, mindgr = 1, vcol = &quot;tomato&quot;, ...) { # Creating a subgraph subnet &lt;- induced_subgraph( net, which(degree(net, mode = &quot;all&quot;) &gt;= mindgr &amp; V(net)$school == schoolid) ) # Fancy graph set.seed(1) plot( subnet, vertex.size = degree(subnet)/5, vertex.label = NA, edge.arrow.size = .25, vertex.color = vcol, ... ) } The function definition: The myplot &lt;- function([arguments]) {[body of the function]} tells R that we are going to create a function called myplot. We declare four specific arguments: net, schoolid, mindgr, and vcol. These are an igraph object, the school id, the minimum degree that vertices must have to be included in the figure, and the color of the vertices. Observe that, compared to other programming languages, R does not require declaring the data types. The ellipsis object, ..., is an especial object in R that allows us to pass other arguments without specifying which. If you take a look at the plot bit in the function body, you will see that we also added .... We use the ellipsis to pass extra arguments (different from the ones that we explicitly defined) directly to plot. In practice, this implies that we can, for example, set the argument edge.arrow.size when calling myplot, even though we did not include it in the function definition! (See ?dotsMethods in R for more details). In the following lines of code, using our new function, we will plot each schools’ network in the same plotting device (window) with the help of the par function, and add legend with the legend: # Plotting all together oldpar &lt;- par(no.readonly = TRUE) par(mfrow = c(2, 3), mai = rep(0, 4), oma= c(1, 0, 0, 0)) myplot(ig_year1, 111, vcol = &quot;tomato&quot;) myplot(ig_year1, 112, vcol = &quot;steelblue&quot;) myplot(ig_year1, 113, vcol = &quot;black&quot;) myplot(ig_year1, 114, vcol = &quot;gold&quot;) myplot(ig_year1, 115, vcol = &quot;white&quot;) par(oldpar) # A fancy legend legend( &quot;bottomright&quot;, legend = c(111, 112, 113, 114, 115), pt.bg = c(&quot;tomato&quot;, &quot;steelblue&quot;, &quot;black&quot;, &quot;gold&quot;, &quot;white&quot;), pch = 21, cex = 1, bty = &quot;n&quot;, title = &quot;School&quot; ) Figure 4.4: All 5 schools in time 1. Again, the graphs exclude isolates. So what happened here? oldpar &lt;- par(no.readonly = TRUE) This line stores the current parameters for plotting. Since we are going to be changing them, we better make sure we are able to go back!. par(mfrow = c(2, 3), mai = rep(0, 4), oma=rep(0, 4)) Here we are setting various things at the same time. mfrow specifies how many figures will be drawn, and in what order. In particular, we are asking the plotting device to make room for 2*3 = 6 figures organized in two rows and three columns drawn by row. mai specifies the size of the margins in inches, setting all margins equal to zero (which is what we are doing now) gives more space to the graph. The same is true for oma. See ?par for more info. myplot(ig_year1, ...) This is simply calling our plotting function. The neat part of this is that, since we set mfrow = c(2, 3), R takes care of distributing the plots in the device. par(oldpar) This line allows us to restore the plotting parameters. Figuring out what is the optimal vertex size is a bit tricky. Without getting too technical, there’s no other way of getting nice vertex size other than just playing with different values of it. A nice solution to this is using netdiffuseR::igraph_vertex_rescale which rescales the vertices so that these keep their aspect ratio to a predefined proportion of the screen.↩ "],["statistical-tests.html", "4.5 Statistical tests", " 4.5 Statistical tests 4.5.1 Is nomination number correlated with indegree? Hypothesis: Individuals that, on average, are among the first nominations of their peers are more popular # Getting all the data in long format edgelist &lt;- as_long_data_frame(ig_year1) %&gt;% as_tibble # Computing indegree (again) and average nomination number # Include &quot;On a scale from one to five how close do you feel&quot; # Also for egocentric friends (A. Friends) indeg_nom_cor &lt;- group_by(edgelist, to, to_name, to_school) %&gt;% summarise( indeg = length(nnom), nom_avg = 1/mean(nnom) ) %&gt;% rename( school = to_school ) ## `summarise()` has grouped output by &#39;to&#39;, &#39;to_name&#39;. You can override using the `.groups` argument. indeg_nom_cor ## # A tibble: 1,561 × 5 ## # Groups: to, to_name [1,561] ## to to_name school indeg nom_avg ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 1110002 111 22 0.222 ## 2 3 1110007 111 7 0.175 ## 3 4 1110013 111 6 0.171 ## 4 5 1110014 111 19 0.134 ## 5 6 1110015 111 3 0.15 ## 6 7 1110020 111 6 0.154 ## 7 9 1110025 111 6 0.214 ## 8 10 1110027 111 13 0.220 ## 9 11 1110029 111 14 0.131 ## 10 12 1110030 111 6 0.222 ## # … with 1,551 more rows # Using pearson&#39;s correlation with(indeg_nom_cor, cor.test(indeg, nom_avg)) ## ## Pearson&#39;s product-moment correlation ## ## data: indeg and nom_avg ## t = -12.254, df = 1559, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.3409964 -0.2504653 ## sample estimates: ## cor ## -0.2963965 save.image(&quot;03.rda&quot;) "],["exponential-random-graph-models.html", "Chapter 5 Exponential Random Graph Models", " Chapter 5 Exponential Random Graph Models I strongly suggest reading the vignette included in the ergm R package vignette(&quot;ergm&quot;, package=&quot;ergm&quot;) The purpose of ERGMs, in a nutshell, is to describe parsimoniously the local selection forces that shape the global structure of a network. To this end, a network dataset, like those depicted in Figure 1, may be considered as the response in a regression model, where the predictors are things like “propensity for individuals of the same sex to form partnerships” or “propensity for individuals to form triangles of partnerships”. In Figure 1(b), for example, it is evident that the individual nodes appear to cluster in groups of the same numerical labels (which turn out to be students’ grades, 7 through 12); thus, an ERGM can help us quantify the strength of this intra-group effect. — (David R. Hunter et al. 2008) Source: Hunter et al. (2008) In a nutshell, we use ERGMs as a parametric interpretation of the distribution of \\(\\mathbf{Y}\\), which takes the canonical form: \\[ \\mbox{Pr}\\left(\\mathbf{Y}=\\mathbf{y}|\\theta, \\mathcal{Y}\\right) = \\frac{\\mbox{exp}\\left\\{\\theta^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\}}{\\kappa\\left(\\theta, \\mathcal{Y}\\right)},\\quad\\mathbf{y}\\in\\mathcal{Y} \\tag{5.1} \\] Where \\(\\theta\\in\\Omega\\subset\\mathbb{R}^q\\) is the vector of model coefficients and \\(\\mathbf{g}(\\mathbf{y})\\) is a q-vector of statistics based on the adjacency matrix \\(\\mathbf{y}\\). Model (5.1) may be expanded by replacing \\(\\mathbf{g}(\\mathbf{y})\\) with \\(\\mathbf{g}(\\mathbf{y}, \\mathbf{X})\\) to allow for additional covariate information \\(\\mathbf{X}\\) about the network. The denominator, \\[ \\kappa\\left(\\theta,\\mathcal{Y}\\right) = \\sum_{\\mathbf{y}\\in\\mathcal{Y}}\\mbox{exp}\\left\\{\\theta^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\} \\] ,is the normalizing factor that ensures that equation (5.1) is a legitimate probability distribution. Even after fixing \\(\\mathcal{Y}\\) to be all the networks that have size \\(n\\), the size of \\(\\mathcal{Y}\\) makes this type of statistical model hard to estimate as there are \\(N = 2^{n(n-1)}\\) possible networks! (David R. Hunter et al. 2008) Recent developments include new forms of dependency structures to take into account more general neighborhood effects. These models relax the one-step Markovian dependence assumptions, allowing investigation of longer-range configurations, such as longer paths in the network or larger cycles (Pattison and Robins 2002). Models for bipartite (Faust and Skvoretz 1999) and tripartite (Mische and Robins 2000) network structures have been developed. (David R. Hunter et al. 2008, 9) References "],["a-naïve-example.html", "5.1 A naïve example", " 5.1 A naïve example In the simplest case, ergm is equivalent to a logistic regression library(ergm) ## Loading required package: network ## ## &#39;network&#39; 1.17.1 (2021-06-12), part of the Statnet Project ## * &#39;news(package=&quot;network&quot;)&#39; for changes since last version ## * &#39;citation(&quot;network&quot;)&#39; for citation information ## * &#39;https://statnet.org&#39; for help, support, and other information ## ## &#39;ergm&#39; 4.1.2 (2021-07-26), part of the Statnet Project ## * &#39;news(package=&quot;ergm&quot;)&#39; for changes since last version ## * &#39;citation(&quot;ergm&quot;)&#39; for citation information ## * &#39;https://statnet.org&#39; for help, support, and other information ## &#39;ergm&#39; 4 is a major update that introduces some backwards-incompatible ## changes. Please type &#39;news(package=&quot;ergm&quot;)&#39; for a list of major ## changes. data(&quot;sampson&quot;) samplike ## Network attributes: ## vertices = 18 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## total edges= 88 ## missing edges= 0 ## non-missing edges= 88 ## ## Vertex attribute names: ## cloisterville group vertex.names ## ## Edge attribute names: ## nominations y &lt;- sort(as.vector(as.matrix(samplike)))[-c(1:18)] glm(y~1, family=binomial(&quot;logit&quot;)) ## ## Call: glm(formula = y ~ 1, family = binomial(&quot;logit&quot;)) ## ## Coefficients: ## (Intercept) ## -0.9072 ## ## Degrees of Freedom: 305 Total (i.e. Null); 305 Residual ## Null Deviance: 367.2 ## Residual Deviance: 367.2 AIC: 369.2 ergm(samplike ~ edges) ## Starting maximum pseudolikelihood estimation (MPLE): ## Evaluating the predictor and response matrix. ## Maximizing the pseudolikelihood. ## Finished MPLE. ## Stopping at the initial estimate. ## Evaluating log-likelihood at the estimate. ## ## Call: ## ergm(formula = samplike ~ edges) ## ## Maximum Likelihood Coefficients: ## edges ## -0.9072 pr &lt;- mean(y) log(pr) - log(1-pr) # Logit function ## [1] -0.9071582 qlogis(pr) ## [1] -0.9071582 "],["estimation-of-ergms.html", "5.2 Estimation of ERGMs", " 5.2 Estimation of ERGMs The ultimate goal is to perform statistical inference on the proposed model. In a standard setting, we would be able to use Maximum-Likelihood-Estimation (MLE), which consists of finding the model parameters \\(\\theta\\) that, given the observed data, maximize the likelihood of the model. For the latter, we generally use Newton’s method. Newton’s method requires been able to compute the log-likelihood of the model, which in ERGMs can be challenging. For ERGMs, since part of the likelihood involves a normalizing constant that is a function of all possible networks, this is not as straightforward as in the regular setting. Because of it, most estimation methods rely on simulations. In statnet, the default estimation method is based on a method proposed by (Geyer and Thompson 1992), Markov-Chain MLE, which uses Markov-Chain Monte Carlo for simulating networks and a modified version of the Newton-Raphson algorithm to estimate the parameters. The idea of MC-MLE for this family of statistical models is that we can approximate the expectation of normalizing constant ratios using the law of large numbers. In particular, the following: \\[ \\begin{aligned} \\frac{\\kappa\\left(\\theta,\\mathcal{Y}\\right)}{\\kappa\\left(\\theta_0,\\mathcal{Y}\\right)} &amp; = % \\frac{% \\sum_{\\mathbf{y}\\in\\mathcal{Y}}\\mbox{exp}\\left\\{\\theta^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\}}{ % \\sum_{\\mathbf{y}\\in\\mathcal{Y}}\\mbox{exp}\\left\\{\\theta_0^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\} } \\\\ &amp; = \\sum_{\\mathbf{y}\\in\\mathcal{Y}}\\left( % \\frac{1}{% \\sum_{\\mathbf{y}\\in\\mathcal{Y}\\mbox{exp}\\left\\{\\theta_0^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\}}% } \\times % \\mbox{exp}\\left\\{\\theta^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\} % \\right) \\\\ &amp; = \\sum_{\\mathbf{y}\\in\\mathcal{Y}}\\left( % \\frac{\\mbox{exp}\\left\\{\\theta_0^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\}}{% \\sum_{\\mathbf{y}\\in\\mathcal{Y}\\mbox{exp}\\left\\{\\theta_0^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\}}% } \\times % \\mbox{exp}\\left\\{(\\theta - \\theta_0)^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\} % \\right) \\\\ &amp; = \\sum_{\\mathbf{y}\\in\\mathcal{Y}}\\left( % \\mbox{Pr}\\left(Y = y|\\mathcal{Y}, \\theta_0\\right) \\times % \\mbox{exp}\\left\\{(\\theta - \\theta_0)^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\} % \\right) \\\\ &amp; = \\mbox{E}_{\\theta_0}\\left(\\mbox{exp}\\left\\{(\\theta - \\theta_0)^{\\mbox{T}}\\mathbf{g}(\\mathbf{y})\\right\\} \\right) \\end{aligned} \\] In particular, the MC-MLE algorithm uses this fact to maximize the ratio of log-likelihoods. The objective function itself can be approximated by simulating \\(m\\) networks from the distribution with parameter \\(\\theta_0\\): \\[ l(\\theta) - l(\\theta_0) \\approx (\\theta - \\theta_0)^{\\mbox{T}}\\mathbf{g}(\\mathbf{y}_{obs}) - \\mbox{log}{\\left[\\frac{1}{m}\\sum_{i = 1}^m\\mbox{exp}\\left\\{(\\theta-\\theta_0)^{\\mbox{T}}\\right\\}\\mathbf{g}(\\mathbf{Y}_i)\\right]} \\] For more details, see (David R. Hunter et al. 2008). A sketch of the algorithm follows: Initialize the algorithm with an initial guess of \\(\\theta\\), call it \\(\\theta^{(t)}\\) (must be a rather OK guess) While (no convergence) do: Using \\(\\theta^{(t)}\\), simulate \\(M\\) networks by means of small changes in the \\(\\mathbf{Y}_{obs}\\) (the observed network). This part is done by using an importance-sampling method which weights each proposed network by it’s likelihood conditional on \\(\\theta^{(t)}\\) With the networks simulated, we can do the Newton step to update the parameter \\(\\theta^{(t)}\\) (this is the iteration part in the ergm package): \\(\\theta^{(t)}\\to\\theta^{(t+1)}\\). If convergence has been reached (which usually means that \\(\\theta^{(t)}\\) and \\(\\theta^{(t + 1)}\\) are not very different), then stop; otherwise, go to step a. For more details see (Lusher, Koskinen, and Robins 2012; Admiraal and Handcock 2006; Snijders 2002; Wang et al. 2009) provides details on the algorithm used by PNet (which is the same as the one used in RSiena). (Lusher, Koskinen, and Robins 2012) provides a short discussion on differences between ergm and PNet. References "],["the-ergm-package.html", "5.3 The ergm package", " 5.3 The ergm package The ergm R package (Handcock et al. 2017) From the previous section:7 library(igraph) library(magrittr) library(dplyr) load(&quot;03.rda&quot;) In this section we will use the ergm package (from the statnet suit of packages (Handcock et al. 2016)) suit, and the intergraph (Bojanowski 2015) package. The latter provides functions to go back and forth between igraph and network objects from the igraph and network packages respectively8 library(ergm) library(intergraph) As a rather important side note, the order in which R packages are loaded matters. Why is this important to mention now? Well, it turns out that at least a couple of functions in the network package have the same name of some functions in the igraph package. When the ergm package is loaded, since it depends on network, it will load the network package first, which will mask some functions in igraph. This becomes evident once you load ergm after loading igraph: The following objects are masked from ‘package:igraph’: add.edges, add.vertices, %c%, delete.edges, delete.vertices, get.edge.attribute, get.edges, get.vertex.attribute, is.bipartite, is.directed, list.edge.attributes, list.vertex.attributes, %s%, set.edge.attribute, set.vertex.attribute What are the implications of this? If you call the function list.edge.attributes for an object of class igraph R will return an error as the first function that matches that name comes from the network package! To avoid this you can use the double colon notation: igraph::list.edge.attributes(my_igraph_object) network::list.edge.attributes(my_network_object) Anyway… Using the asNetwork function, we can coerce the igraph object into a network object so we can use it with the ergm function: # Creating the new network network_111 &lt;- intergraph::asNetwork(ig_year1_111) # Running a simple ergm (only fitting edge count) ergm(network_111 ~ edges) ## [1] &quot;Warning: This network contains loops&quot; ## Starting maximum pseudolikelihood estimation (MPLE): ## Evaluating the predictor and response matrix. ## Maximizing the pseudolikelihood. ## Finished MPLE. ## Stopping at the initial estimate. ## Evaluating log-likelihood at the estimate. ## ## Call: ## ergm(formula = network_111 ~ edges) ## ## Maximum Likelihood Coefficients: ## edges ## -4.734 So what happened here! We got a warning. It turns out that our network has loops (didn’t thought about it before!). Let’s take a look at that with the which_loop function E(ig_year1_111)[which_loop(ig_year1_111)] ## + 1/2638 edge from ce68f58 (vertex names): ## [1] 1110111-&gt;1110111 We can get rid of these using the igraph::-.igraph. Let’s remove the isolates using the same operator # Creating the new network network_111 &lt;- ig_year1_111 # Removing loops network_111 &lt;- network_111 - E(network_111)[which(which_loop(network_111))] # Removing isolates network_111 &lt;- network_111 - which(degree(network_111, mode = &quot;all&quot;) == 0) # Converting the network network_111 &lt;- intergraph::asNetwork(network_111) asNetwork(simplify(ig_year1_111)) ig_year1_111 %&gt;% simplify %&gt;% asNetwork A problem that we have on this data is the fact that some vertices have missing values in the variables hispanic, female1, and eversmk1. For now, we will proceed by imputing values based on the avareges: for (v in c(&quot;hispanic&quot;, &quot;female1&quot;, &quot;eversmk1&quot;)) { tmpv &lt;- network_111 %v% v tmpv[is.na(tmpv)] &lt;- mean(tmpv, na.rm = TRUE) &gt; .5 network_111 %v% v &lt;- tmpv } References "],["running-ergms.html", "5.4 Running ERGMs", " 5.4 Running ERGMs Proposed workflow: Estimate the simplest model, adding one variable at a time. After each estimation, run the mcmc.diagnostics function to see how good (or bad) behaved the chains are. Run the gof function and verify how good the model matches the network’s structural statistics. What to use: control.ergms: Maximum number of iteration, seed for Pseudo-RNG, how many cores ergm.constraints: Where to sample the network from. Gives stability and (in some cases) faster convergence as by constraining the model you are reducing the sample size. Here is an example of a couple of models that we could compare9 ans0 &lt;- ergm( network_111 ~ edges + nodematch(&quot;hispanic&quot;) + nodematch(&quot;female1&quot;) + nodematch(&quot;eversmk1&quot;) + mutual , constraints = ~bd(maxout = 19), control = control.ergm( seed = 1, MCMLE.maxit = 10, parallel = 4, CD.maxit = 10 ) ) So what are we doing here: The model is controlling for: edges Number of edges in the network (as opposed to its density) nodematch(\"some-variable-name-here\") Includes a term that controls for homophily/heterophily mutual Number of mutual connections between \\((i, j), (j, i)\\). This can be related to, for example, triadic closure. For more on control parameters, see (Morris, Handcock, and Hunter 2008). ans1 &lt;- ergm( network_111 ~ edges + nodematch(&quot;hispanic&quot;) + nodematch(&quot;female1&quot;) + nodematch(&quot;eversmk1&quot;) , constraints = ~bd(maxout = 19), control = control.ergm( seed = 1, MCMLE.maxit = 10, parallel = 4, CD.maxit = 10 ) ) This example takes longer to compute ans2 &lt;- ergm( network_111 ~ edges + nodematch(&quot;hispanic&quot;) + nodematch(&quot;female1&quot;) + nodematch(&quot;eversmk1&quot;) + mutual + balance , constraints = ~bd(maxout = 19), control = control.ergm( seed = 1, MCMLE.maxit = 10, parallel = 4, CD.maxit = 10 ) ) Now, a nice trick to see all regressions in the same table, we can use the texreg package (Leifeld 2013) which supports ergm ouputs! library(texreg) ## Version: 1.38.5 ## Date: 2022-03-03 ## Author: Philip Leifeld (University of Essex) ## ## Consider submitting praise using the praise or praise_interactive functions. ## Please cite the JSS article in your publications -- see citation(&quot;texreg&quot;). ## ## Attaching package: &#39;texreg&#39; ## The following object is masked from &#39;package:magrittr&#39;: ## ## extract screenreg(list(ans0, ans1, ans2)) ## ## =============================================================== ## Model 1 Model 2 Model 3 ## --------------------------------------------------------------- ## edges -5.63 *** -5.49 *** -5.60 *** ## (0.05) (0.06) (0.06) ## nodematch.hispanic 0.22 *** 0.30 *** 0.22 *** ## (0.04) (0.05) (0.04) ## nodematch.female1 0.87 *** 1.17 *** 0.87 *** ## (0.04) (0.05) (0.04) ## nodematch.eversmk1 0.33 *** 0.45 *** 0.34 *** ## (0.04) (0.04) (0.04) ## mutual 4.10 *** 1.75 *** ## (0.07) (0.14) ## balance 0.01 *** ## (0.00) ## --------------------------------------------------------------- ## AIC -40017.80 -37511.87 -39989.59 ## BIC -39967.46 -37471.60 -39929.18 ## Log Likelihood 20013.90 18759.94 20000.79 ## =============================================================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 Or, if you are using rmarkdown, you can export the results using LaTeX or html, let’s try the latter to see how it looks like here: library(texreg) htmlreg(list(ans0, ans1, ans2)) Statistical models   Model 1 Model 2 Model 3 edges -5.63*** -5.49*** -5.60***   (0.05) (0.06) (0.06) nodematch.hispanic 0.22*** 0.30*** 0.22***   (0.04) (0.05) (0.04) nodematch.female1 0.87*** 1.17*** 0.87***   (0.04) (0.05) (0.04) nodematch.eversmk1 0.33*** 0.45*** 0.34***   (0.04) (0.04) (0.04) mutual 4.10***   1.75***   (0.07)   (0.14) balance     0.01***       (0.00) AIC -40017.80 -37511.87 -39989.59 BIC -39967.46 -37471.60 -39929.18 Log Likelihood 20013.90 18759.94 20000.79 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 References "],["model-goodness-of-fit.html", "5.5 Model Goodness-of-Fit", " 5.5 Model Goodness-of-Fit In raw terms, once each chain has reach stationary distribution, we can say that there are no problems with autocorrelation and that each sample point is iid. This implies that, since we are running the model with more than 1 chain, we can use all the samples (chains) as a single dataset. Recent changes in the ergm estimation algorithm mean that these plots can no longer be used to ensure that the mean statistics from the model match the observed network statistics. For that functionality, please use the GOF command: gof(object, GOF=~model). —?ergm::mcmc.diagnostics Since ans0 is the one model which did best, let’s take a look at it’s GOF statistics. First, lets see how the MCMC did. For this we can use the mcmc.diagnostics function including in the package. This function is actually a wrapper of a couple of functions from the coda package (Plummer et al. 2006) which is called upon the $sample object which holds the centered statistics from the sampled networks. This last point is important to consider since at first look it can be confusing to look at the $sample object since it neither matches the observed statistics, nor the coefficients. When calling the function mcmc.diagnostics(ans0, centered = FALSE), you will see a lot of output including a couple of plots showing the trace and posterior distribution of the uncentered statistics (centered = FALSE). In the next code chunks we will reproduce the output from the mcmc.diagnostics function step by step using the coda package. First we need to uncenter the sample object: # Getting the centered sample sample_centered &lt;- ans0$sample # Getting the observed statistics and turning it into a matrix so we can add it # to the samples observed &lt;- summary(ans0$formula) observed &lt;- matrix( observed, nrow = nrow(sample_centered[[1]]), ncol = length(observed), byrow = TRUE ) # Now we uncenter the sample sample_uncentered &lt;- lapply(sample_centered, function(x) { x + observed }) # We have to make it an mcmc.list object sample_uncentered &lt;- coda::mcmc.list(sample_uncentered) Under the hood: Empirical means and sd, and quantiles: summary(sample_uncentered) ## ## Iterations = 1769472:10944512 ## Thinning interval = 65536 ## Number of chains = 4 ## Sample size per chain = 141 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## edges 2485 60.26 2.5372 3.753 ## nodematch.hispanic 1838 51.25 2.1578 3.662 ## nodematch.female1 1888 52.78 2.2224 3.779 ## nodematch.eversmk1 1759 50.82 2.1400 3.072 ## mutual 493 23.40 0.9855 1.967 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## edges 2373 2444 2482 2530 2612 ## nodematch.hispanic 1736 1803 1839 1872 1947 ## nodematch.female1 1791 1851 1885 1923 1993 ## nodematch.eversmk1 1662 1725 1758 1794 1858 ## mutual 449 476 493 509 537 Cross correlation: coda::crosscorr(sample_uncentered) ## edges nodematch.hispanic nodematch.female1 ## edges 1.0000000 0.8657369 0.8851587 ## nodematch.hispanic 0.8657369 1.0000000 0.7713632 ## nodematch.female1 0.8851587 0.7713632 1.0000000 ## nodematch.eversmk1 0.8445651 0.7122693 0.7572735 ## mutual 0.7726517 0.6801783 0.7482026 ## nodematch.eversmk1 mutual ## edges 0.8445651 0.7726517 ## nodematch.hispanic 0.7122693 0.6801783 ## nodematch.female1 0.7572735 0.7482026 ## nodematch.eversmk1 1.0000000 0.6873242 ## mutual 0.6873242 1.0000000 Autocorrelation: For now, we will only look at autocorrelation for chain one. Autocorrelation should be small (in a general MCMC setting). If autocorrelation is high, then it means that your sample is not idd (no Markov property). A way out to solve this is thinning the sample. coda::autocorr(sample_uncentered)[[1]] ## , , edges ## ## edges nodematch.hispanic nodematch.female1 nodematch.eversmk1 ## Lag 0 1.000000000 0.861920590 0.90235072 0.86215333 ## Lag 65536 0.415060923 0.326775063 0.43751588 0.38274418 ## Lag 327680 0.063993999 0.002238453 0.09094189 0.05143792 ## Lag 655360 0.002497326 -0.105210070 -0.02414091 0.00143358 ## Lag 3276800 0.026845190 0.068616366 0.03686125 0.03652383 ## mutual ## Lag 0 0.785264416 ## Lag 65536 0.428519050 ## Lag 327680 0.074020671 ## Lag 655360 0.009422505 ## Lag 3276800 0.018126669 ## ## , , nodematch.hispanic ## ## edges nodematch.hispanic nodematch.female1 nodematch.eversmk1 ## Lag 0 0.86192059 1.000000000 0.76137201 0.74623272 ## Lag 65536 0.32680263 0.336764054 0.30353156 0.32690588 ## Lag 327680 0.05778076 0.004465856 0.07267341 0.03757479 ## Lag 655360 0.07704457 0.024226503 0.03252125 0.08420548 ## Lag 3276800 -0.02970399 0.021278122 -0.02753467 -0.03018601 ## mutual ## Lag 0 0.70578514 ## Lag 65536 0.35558587 ## Lag 327680 0.05282736 ## Lag 655360 0.08176601 ## Lag 3276800 -0.07743174 ## ## , , nodematch.female1 ## ## edges nodematch.hispanic nodematch.female1 nodematch.eversmk1 ## Lag 0 0.902350724 0.76137201 1.00000000 0.77769826 ## Lag 65536 0.453418914 0.37756721 0.51290498 0.41954866 ## Lag 327680 0.055464012 -0.01058737 0.09841770 0.04272154 ## Lag 655360 0.009910833 -0.06123858 -0.03186870 0.04679847 ## Lag 3276800 0.004163166 0.04057544 0.01548719 -0.01288236 ## mutual ## Lag 0 0.76981085 ## Lag 65536 0.46327442 ## Lag 327680 0.03629824 ## Lag 655360 0.01987496 ## Lag 3276800 -0.00949882 ## ## , , nodematch.eversmk1 ## ## edges nodematch.hispanic nodematch.female1 nodematch.eversmk1 ## Lag 0 0.86215333 0.746232721 0.77769826 1.000000000 ## Lag 65536 0.37539678 0.297591397 0.41717478 0.448697559 ## Lag 327680 0.02105523 -0.040132752 0.03760486 0.019124328 ## Lag 655360 0.04566425 0.003387581 0.04761067 -0.006388743 ## Lag 3276800 0.05048735 0.084790008 0.07108989 0.045582057 ## mutual ## Lag 0 0.7053009595 ## Lag 65536 0.4020746950 ## Lag 327680 0.0183308894 ## Lag 655360 0.0840948296 ## Lag 3276800 0.0009713556 ## ## , , mutual ## ## edges nodematch.hispanic nodematch.female1 nodematch.eversmk1 ## Lag 0 0.78526442 0.70578514 0.769810849 0.70530096 ## Lag 65536 0.50645801 0.44741607 0.532817503 0.47751208 ## Lag 327680 0.12979152 0.06061696 0.147380566 0.10930214 ## Lag 655360 -0.06393205 -0.13217821 -0.008121728 -0.03814393 ## Lag 3276800 -0.01707605 0.03244214 -0.023750630 0.02781638 ## mutual ## Lag 0 1.000000000 ## Lag 65536 0.580271013 ## Lag 327680 0.091309576 ## Lag 655360 -0.003521212 ## Lag 3276800 -0.025558756 Geweke Diagnostic: From the function’s help file: “If the samples are drawn from the stationary distribution of the chain, the two means are equal and Geweke’s statistic has an asymptotically standard normal distribution. […] The Z-score is calculated under the assumption that the two parts of the chain are asymptotically independent, which requires that the sum of frac1 and frac2 be strictly less than 1.”\" —?coda::geweke.diag Let’s take a look at a single chain: coda::geweke.diag(sample_uncentered)[[1]] ## ## Fraction in 1st window = 0.1 ## Fraction in 2nd window = 0.5 ## ## edges nodematch.hispanic nodematch.female1 nodematch.eversmk1 ## -0.7115 -1.7204 -0.1841 0.6952 ## mutual ## -1.2891 (not included) Gelman Diagnostic: From the function’s help file: Gelman and Rubin (1992) propose a general approach to monitoring convergence of MCMC output in which m &gt; 1 parallel chains are run with starting values that are overdispersed relative to the posterior distribution. Convergence is diagnosed when the chains have ‘forgotten’ their initial values, and the output from all chains is indistinguishable. The gelman.diag diagnostic is applied to a single variable from the chain. It is based a comparison of within-chain and between-chain variances, and is similar to a classical analysis of variance. —?coda::gelman.diag As a difference from the previous diagnostic statistic, this uses all chains simulatenously: coda::gelman.diag(sample_uncentered) ## Potential scale reduction factors: ## ## Point est. Upper C.I. ## edges 1.03 1.10 ## nodematch.hispanic 1.03 1.10 ## nodematch.female1 1.05 1.14 ## nodematch.eversmk1 1.04 1.12 ## mutual 1.05 1.14 ## ## Multivariate psrf ## ## 1.05 As a rule of thumb, values that are in the \\([.9,1.1]\\) are good. One nice feature of the mcmc.diagnostics function is the nice trace and posterior distribution plots that it generates. If you have the R package latticeExtra (Sarkar and Andrews 2016), the function will override the default plots used by coda::plot.mcmc and use lattice instead, creating a nicer looking plots. The next code chunk calls the mcmc.diagnostic function, but we suppress the rest of the output (see figure ??). # [2022-03-13] This line is failing for what it could be an ergm bug # mcmc.diagnostics(ans0, center = FALSE) # Suppressing all the output If we called the function mcmc.diagnostics, this message appears at the end: MCMC diagnostics shown here are from the last round of simulation, prior to computation of final parameter estimates. Because the final estimates are refinements of those used for this simulation run, these diagnostics may understate model performance. To directly assess the performance of the final model on in-model statistics, please use the GOF command: gof(ergmFitObject, GOF=~model). —mcmc.diagnostics(ans0) Not that bad (although the mutual term could do better)!10 First, observe that in the figure we see four different lines; why is that? Since we were running in parallel using four cores, the algorithm ran four chains of the MCMC algorithm. An eyeball test is to see if all the chains moved at about the same place; in such a case, we can start thinking about model convergence from the MCMC perspective. Once we are sure to have reach convergence on the MCMC algorithm, we can start thinking about how well does our model predicts the observed network’s proterties. Besides the statistics that define our ERGM, the gof function’s default behavior show GOF for: In degree distribution, Out degree distribution, Edge-wise shared partners, and Geodesics Let’s take a look at it # Computing and printing GOF estatistics ans_gof &lt;- gof(ans0) ans_gof ## ## Goodness-of-fit for in-degree ## ## obs min mean max MC p-value ## idegree0 13 0 1.43 5 0.00 ## idegree1 34 2 8.47 16 0.00 ## idegree2 37 13 22.53 33 0.00 ## idegree3 48 29 42.25 60 0.38 ## idegree4 37 45 57.78 76 0.00 ## idegree5 47 44 66.51 88 0.02 ## idegree6 42 44 64.18 79 0.00 ## idegree7 39 40 53.58 68 0.00 ## idegree8 35 23 39.58 53 0.50 ## idegree9 21 18 27.43 40 0.28 ## idegree10 12 9 16.38 23 0.34 ## idegree11 19 1 8.82 16 0.00 ## idegree12 4 0 4.63 12 1.00 ## idegree13 7 0 2.12 7 0.02 ## idegree14 6 0 1.26 5 0.00 ## idegree15 3 0 0.44 2 0.00 ## idegree16 4 0 0.37 2 0.00 ## idegree17 3 0 0.14 1 0.00 ## idegree18 3 0 0.06 1 0.00 ## idegree19 2 0 0.02 1 0.00 ## idegree20 1 0 0.00 0 0.00 ## idegree21 0 0 0.02 1 1.00 ## idegree22 1 0 0.00 0 0.00 ## ## Goodness-of-fit for out-degree ## ## obs min mean max MC p-value ## odegree0 4 0 1.56 5 0.10 ## odegree1 28 2 8.23 17 0.00 ## odegree2 45 11 22.52 33 0.00 ## odegree3 50 23 40.11 56 0.10 ## odegree4 54 43 59.27 76 0.52 ## odegree5 62 50 67.39 93 0.48 ## odegree6 40 45 63.79 79 0.00 ## odegree7 28 32 54.74 73 0.00 ## odegree8 13 27 39.88 50 0.00 ## odegree9 16 14 26.17 43 0.04 ## odegree10 20 5 16.30 25 0.36 ## odegree11 8 4 8.99 17 0.76 ## odegree12 11 1 4.73 10 0.00 ## odegree13 13 0 2.54 8 0.00 ## odegree14 6 0 1.00 3 0.00 ## odegree15 6 0 0.45 3 0.00 ## odegree16 7 0 0.21 2 0.00 ## odegree17 4 0 0.10 2 0.00 ## odegree18 3 0 0.02 1 0.00 ## ## Goodness-of-fit for edgewise shared partner ## ## obs min mean max MC p-value ## esp0 1032 1979 2193.78 2313 0 ## esp1 755 166 235.04 423 0 ## esp2 352 2 16.24 83 0 ## esp3 202 0 0.88 5 0 ## esp4 79 0 0.02 1 0 ## esp5 36 0 0.00 0 0 ## esp6 14 0 0.00 0 0 ## esp7 4 0 0.00 0 0 ## esp8 1 0 0.00 0 0 ## ## Goodness-of-fit for minimum geodesic distance ## ## obs min mean max MC p-value ## 1 2475 2329 2445.96 2573 0.52 ## 2 10672 12419 13611.54 15000 0.00 ## 3 31134 49756 55207.67 60888 0.00 ## 4 50673 77398 79886.41 81893 0.00 ## 5 42563 15561 20575.07 26629 0.00 ## 6 18719 491 1265.91 2285 0.00 ## 7 4808 4 38.99 178 0.00 ## 8 822 0 0.77 14 0.00 ## 9 100 0 0.03 1 0.00 ## 10 7 0 0.00 0 0.00 ## Inf 12333 0 1273.65 3321 0.00 ## ## Goodness-of-fit for model statistics ## ## obs min mean max MC p-value ## edges 2475 2329 2445.96 2573 0.52 ## nodematch.hispanic 1832 1740 1826.07 1978 0.86 ## nodematch.female1 1879 1760 1860.64 1958 0.76 ## nodematch.eversmk1 1755 1646 1733.50 1814 0.46 ## mutual 486 434 472.02 498 0.42 # Plotting GOF statistics plot(ans_gof) Try the following configuration instead ans0_bis &lt;- ergm( network_111 ~ edges + nodematch(&quot;hispanic&quot;) + nodematch(&quot;female1&quot;) + mutual + esp(0:3) + idegree(0:10) , constraints = ~bd(maxout = 19), control = control.ergm( seed = 1, MCMLE.maxit = 15, parallel = 4, CD.maxit = 15, MCMC.samplesize = 2048*4, MCMC.burnin = 30000, MCMC.interval = 2048*4 ) ) Increase the sample size, so the curves are smoother, longer intervals (thinning), which reduces autocorrelation, and a larger burin. All this together to improve the Gelman test statistic. We also added idegree from 0 to 10, and esp from 0 to 3 to explicitly match those statistics in our model. knitr::include_graphics(&quot;awful-chains.png&quot;) Figure 5.1: An example of a terrible ERGM (no convergence at all). Also, a good example of why running multiple chains can be useful References "],["more-on-mcmc-convergence.html", "5.6 More on MCMC convergence", " 5.6 More on MCMC convergence For more on this issue, I recommend reviewing chapter 1 and chapter 6 from the Handbook of MCMC (Brooks et al. 2011). Both chapters are free to download from the book’s website. For GOF take a look at section 6 of ERGM 2016 Sunbelt tutorial, and for a more technical review, you can take a look at (David R Hunter, Goodreau, and Handcock 2008). References "],["mathematical-interpretation.html", "5.7 Mathematical Interpretation", " 5.7 Mathematical Interpretation One of the most critical parts of statistical modeling is interpreting the results, if not the most important. In the case of ERGMs, a key aspect is based on change statistics. Suppose that we would like to know how likely the tie \\(y_{ij}\\) is to happen, given the rest of the network. We can compute such probabilities using what literature sometimes describes as the Gibbs-sampler. In particular, the log-odds of the \\(ij\\) tie ocurring conditional on the rest of the network can be written as: \\[\\begin{equation} \\mbox{logit}\\left({\\mathbb{P}\\left(y_{ij} = 1|y_{-ij}\\right) }\\right) = {\\theta}^\\mathbf{t}\\Delta\\delta\\left(y_{ij}:0\\to 1\\right), \\end{equation}\\] with \\(\\delta\\left(y_{ij}:0\\to 1\\right)\\equiv s\\left(\\mathbf{y}\\right)_{\\mbox{ij}}^+ - s\\left(\\mathbf{y}\\right)_{\\mbox{ij}}^-\\) as the vector of change statistics, in other words, the difference between the sufficient statistics when \\(y_{ij}=1\\) and its value when \\(y_{ij} = 0\\). To show this, we write the following: \\[\\begin{align*} {\\mathbb{P}\\left(y_{ij} = 1|y_{-ij}\\right) } &amp; = % \\frac{{\\mathbb{P}\\left(y_{ij} = 1, x_{-ij}\\right) }}{% {\\mathbb{P}\\left(y_{ij} = 1, y_{-ij}\\right) } + {\\mathbb{P}\\left(y_{ij} = 0, y_{-ij}\\right) }} \\\\ &amp; = \\frac{\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^+_{ij}\\right\\}}{% \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^+_{ij}\\right\\} + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^-_{ij}\\right\\}} \\end{align*}\\] Applying the logit function to the previous equation, we obtain: \\[\\begin{align*} &amp; = \\mbox{log}\\left\\{\\frac{\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^+_{ij}\\right\\}}{% \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^+_{ij}\\right\\} + % \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^-_{ij}\\right\\}}\\right\\} - % \\mbox{log}\\left\\{ % \\frac{\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^-_{ij}\\right\\}}{% \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^+_{ij}\\right\\} + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^-_{ij}\\right\\}}% \\right\\} \\\\ &amp; = \\mbox{log}\\left\\{\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^+_{ij}\\right\\}\\right\\} - \\mbox{log}\\left\\{\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)^-_{ij}\\right\\}\\right\\} \\\\ &amp; = {\\theta}^\\mathbf{t}\\left(s\\left(\\mathbf{y}\\right)^+_{ij} - s\\left(\\mathbf{y}\\right)^-_{ij}\\right) \\\\ &amp; = {\\theta}^\\mathbf{t}\\Delta\\delta\\left(y_{ij}:0\\to 1\\right) \\end{align*}\\] Henceforth, the conditional probability of node \\(n\\) gaining function \\(k\\) can be written as: \\[\\begin{equation} {\\mathbb{P}\\left(y_{ij} = 1|y_{-ij}\\right) } = \\frac{1}{1 + \\mbox{exp}\\left\\{-{\\theta}^\\mathbf{t}\\Delta\\delta\\left(y_{ij}:0\\to 1\\right)\\right\\}} \\end{equation}\\] i.e., a logistic probability. "],["markov-independence.html", "5.8 Markov independence", " 5.8 Markov independence The challenge of analyzing networks is their interdependent nature. Nonetheless, in the absence of such interdependence, ERGMs are equivalent to logistic regression. Conceptually, if all the statistics included in the model do not involve two or more dyads, then the model is non-Markovian in the sense of Markov graphs. Mathematically, to see this, it suffices to show that the ERGM probability can be written as the product of each dyads’ probabilities. \\[\\begin{equation*} {\\mathbb{P}\\left(\\mathbf{y} | \\theta\\right) } = \\frac{\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)\\right\\}}{\\sum_{y}\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)\\right\\}} = \\frac{\\prod_{ij}\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{ij}\\right\\}}{\\sum_{y}\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)\\right\\}} \\end{equation*}\\] Where \\(s\\left(\\right)_{ij}\\) is a function such that \\(s\\left(\\mathbf{y}\\right) = \\sum_{ij}{s\\left(\\mathbf{y}\\right)_{ij}}\\). We now need to deal with the normalizing constant. To see how that can be saparated, let’s start from the result: \\[\\begin{align*} &amp; =\\prod_{ij}\\left(1 + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{ij}\\right\\}\\right) \\\\ &amp; = \\left(1 + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{11}\\right\\}\\right)\\left(1 + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{12}\\right\\}\\right)\\dots\\left(1 + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{nn}\\right\\}\\right) \\\\ &amp; = 1 + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{11}\\right\\} + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{11}\\right\\}\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{12}\\right\\} + \\dots + \\prod_{ij}\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{ij}\\right\\} \\\\ &amp; = 1 + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{11}\\right\\} + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}\\left(s\\left(\\mathbf{y}\\right)_{11} + s\\left(\\mathbf{y}\\right)_{12}\\right)\\right\\} + \\dots + \\prod_{ij}\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{ij}\\right\\} \\\\ &amp; = \\sum_{\\mathbf{y}\\in\\mathcal{Y}}\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)\\right\\} \\end{align*}\\] Where the last equality follows from \\(s\\left(\\mathbf{y}\\right) = \\sum_{ij}{s\\left(\\mathbf{y}\\right)_{ij}}\\). This way, we can now write: \\[\\begin{equation} \\frac{\\prod_{ij}\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{ij}\\right\\}}{\\sum_{y}\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)\\right\\}} = \\prod_{ij}\\frac{\\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{ij}\\right\\}}{1 + \\mbox{exp}\\left\\{{\\theta}^\\mathbf{t}s\\left(\\mathbf{y}\\right)_{ij}\\right\\}} \\end{equation}\\] Related to this, block-diagonal ERGMs can be estimated as independent models, one per block. To see more about this, read (SNIJDERS 2010). Likewise, since independence depends–pun intended–on partitioning the objective function, as pointed by Snijders, non-linear functions make the model dependent, e.g., \\(s\\left(\\mathbf{y}\\right) = \\sqrt{\\sum_{ij}y_{ij}}\\), the square root of the edgecount is no longer a bernoulli graph. References "],["using-constraints-in-ergms.html", "Chapter 6 Using constraints in ERGMs", " Chapter 6 Using constraints in ERGMs Exponential Random Graph Models [ERGMs] can represent a variety of network classes. We often look at ``regular’’ social networks like students in schools, colleagues in a workplace, or families. Nonetheless, some social networks we study have features that restrict how connections can occur. Typical examples are bi-partite graphs and multilevel networks. There are two classes of vertices in bi-partite networks, and ties can only occur between classes. On the other hand, Multilevel networks may feature multiple classes with inter-class ties somewhat restricted. In both cases, structural constraints exist, meaning that some configurations may not be plausible. The ergm R package has built-in capabilities to deal with some of these cases. Nonetheless, we can specify models with arbitrary structural constraints built into the model. The key is in using offset terms. "],["example-1-interlocking-egos-and-disconnected-alters.html", "6.1 Example 1: Interlocking egos and disconnected alters", " 6.1 Example 1: Interlocking egos and disconnected alters Imagine that we have two sets of vertices. The first, group E, are egos part of an egocentric study. The second group, called A, is composed of people mentioned by egos in E but were not surveyed. Assume that individuals in A can only connect to individuals in E; moreover, individuals in E have no restrictions connecting to each other. In other words, only two types of ties exist: A-A and A-E. The question is now, how can we enforce such a constraint in an ERGM? Using offsets, and in particular, setting coefficients to -Inf provides an easy way to restrict the support set of ERGMs. For example, if we wanted to constrain the support to include networks with no triangles, we would add the term offset(triangle) and use the option offset.coef = -Inf to indicate that realizations including triangles are not possible. Using R: ergm(net ~ edges + offset(triangle), offset.coef = -Inf) In this model, a Bernoulli graph, we reduce the sample space to networks with no triangles. In our example, such statistic should only take non-zero values whenever ties within the A class happen. We can use the nodematch() term to do that. Formally \\[ \\text{NodeMatch}(x) = \\sum_{i,j} y_{ij} \\mathbf{1}({x_{i} = x_{j}}) \\] This statistic will sum over all ties in which source (\\(i\\)) and target (\\(j\\))’s \\(X\\) attribute is equal. One way to make this happen is by creating an auxiliary variable that equals, e.g., 0 for all vertices in A, and a unique value different from zero otherwise. For example, if we had 2 As and three Es, the data would look something like this: \\(\\{0,0,1,2,3\\}\\). The following code block creates an empty graph with 50 nodes, 10 of which are in group E (ego). library(ergm) ## Loading required package: network ## ## &#39;network&#39; 1.17.1 (2021-06-12), part of the Statnet Project ## * &#39;news(package=&quot;network&quot;)&#39; for changes since last version ## * &#39;citation(&quot;network&quot;)&#39; for citation information ## * &#39;https://statnet.org&#39; for help, support, and other information ## ## &#39;ergm&#39; 4.1.2 (2021-07-26), part of the Statnet Project ## * &#39;news(package=&quot;ergm&quot;)&#39; for changes since last version ## * &#39;citation(&quot;ergm&quot;)&#39; for citation information ## * &#39;https://statnet.org&#39; for help, support, and other information ## &#39;ergm&#39; 4 is a major update that introduces some backwards-incompatible ## changes. Please type &#39;news(package=&quot;ergm&quot;)&#39; for a list of major ## changes. library(sna) ## Loading required package: statnet.common ## ## Attaching package: &#39;statnet.common&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## attr, order ## sna: Tools for Social Network Analysis ## Version 2.6 created on 2020-10-5. ## copyright (c) 2005, Carter T. Butts, University of California-Irvine ## For citation information, type citation(&quot;sna&quot;). ## Type help(package=&quot;sna&quot;) to get started. n &lt;- 50 n_egos &lt;- 10 net &lt;- as.network(matrix(0, ncol = n, nrow = n), directed = TRUE) # Let&#39;s assing the groups net %v% &quot;is.ego&quot; &lt;- c(rep(TRUE, n_egos), rep(FALSE, n - n_egos)) net %v% &quot;is.ego&quot; ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [49] FALSE FALSE gplot(net, vertex.col = net %v% &quot;is.ego&quot;) To create the auxiliary variable, we will use the following function: # Function that creates an aux variable for the ergm model make_aux_var &lt;- function(my_net, is_ego_dummy) { n_vertex &lt;- length(my_net %v% is_ego_dummy) n_ego_ &lt;- sum(my_net %v% is_ego_dummy) # Creating an auxiliary variable to identify the non-informant non-informant ties my_net %v% &quot;aux_var&quot; &lt;- ifelse( !my_net %v% is_ego_dummy, 0, 1:(n_vertex - n_ego_) ) my_net } Calling the function in our data results in the following: net &lt;- make_aux_var(net, &quot;is.ego&quot;) # Taking a look over the first 15 rows of data cbind( Is_Ego = net %v% &quot;is.ego&quot;, Aux = net %v% &quot;aux_var&quot; ) |&gt; head(n = 15) ## Is_Ego Aux ## [1,] 1 1 ## [2,] 1 2 ## [3,] 1 3 ## [4,] 1 4 ## [5,] 1 5 ## [6,] 1 6 ## [7,] 1 7 ## [8,] 1 8 ## [9,] 1 9 ## [10,] 1 10 ## [11,] 0 0 ## [12,] 0 0 ## [13,] 0 0 ## [14,] 0 0 ## [15,] 0 0 We can now use this data to simulate a network in which ties between A-class vertices are not possible: set.seed(2828) net_sim &lt;- simulate(net ~ edges + nodematch(&quot;aux_var&quot;), coef = c(-3.0, -Inf)) gplot(net_sim, vertex.col = net_sim %v% &quot;is.ego&quot;) As you can see, this network has only ties of the type E-E and A-E. We can double-check by (i) looking at the counts and (ii) visualizing each induced-subgraph separately: summary(net_sim ~ edges + nodematch(&quot;aux_var&quot;)) ## edges nodematch.aux_var ## 49 0 net_of_alters &lt;- get.inducedSubgraph( net_sim, which((net_sim %v% &quot;aux_var&quot;) == 0) ) net_of_egos &lt;- get.inducedSubgraph( net_sim, which((net_sim %v% &quot;aux_var&quot;) != 0) ) # Counts summary(net_of_alters ~ edges + nodematch(&quot;aux_var&quot;)) ## edges nodematch.aux_var ## 0 0 summary(net_of_egos ~ edges + nodematch(&quot;aux_var&quot;)) ## edges nodematch.aux_var ## 1 0 # Figures op &lt;- par(mfcol = c(1, 2)) gplot(net_of_alters, vertex.col = net_of_alters %v% &quot;is.ego&quot;, main = &quot;A&quot;) gplot(net_of_egos, vertex.col = net_of_egos %v% &quot;is.ego&quot;, main = &quot;E&quot;) par(op) Now, to fit an ERGM with this constraint, we simply need to make use of the offset terms. Here is an example: ans &lt;- ergm( net_sim ~ edges + offset(nodematch(&quot;aux_var&quot;)), # The model (notice the offset) offset.coef = -Inf # The offset coefficient ) ## Starting maximum pseudolikelihood estimation (MPLE): ## Evaluating the predictor and response matrix. ## Maximizing the pseudolikelihood. ## Finished MPLE. ## Stopping at the initial estimate. ## Evaluating log-likelihood at the estimate. summary(ans) ## Call: ## ergm(formula = net_sim ~ edges + offset(nodematch(&quot;aux_var&quot;)), ## offset.coef = -Inf) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -2.843 0.147 0 -19.34 &lt;1e-04 *** ## offset(nodematch.aux_var) -Inf 0.000 0 -Inf &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 3396 on 2450 degrees of freedom ## Residual Deviance: 2542 on 2448 degrees of freedom ## ## AIC: 2544 BIC: 2550 (Smaller is better. MC Std. Err. = 0) ## ## The following terms are fixed by offset and are not estimated: ## offset(nodematch.aux_var) This ERGM model–which by the way only featured dyadic-independent terms, and thus can be reduced to a logistic regression–restricts the support by excluding all networks in which ties within the class A exists. To finalize, let’s look at a few simulations based on this model: set.seed(1323) op &lt;- par(mfcol = c(2,2), mar = rep(1, 4)) for (i in 1:4) { gplot(simulate(ans), vertex.col = net %v% &quot;is.ego&quot;, vertex.cex = 2) box() } par(op) All networks with no ties between A nodes. "],["separable-temporal-exponential-family-random-graph-models.html", "Chapter 7 (Separable) Temporal Exponential Family Random Graph Models", " Chapter 7 (Separable) Temporal Exponential Family Random Graph Models This tutorial is great! https://statnet.org/trac/raw-attachment/wiki/Sunbelt2016/tergm_tutorial.pdf "],["hypothesis-testing-in-networks.html", "Chapter 8 Hypothesis testing in networks", " Chapter 8 Hypothesis testing in networks Overall, there are many ways in which we can see hypothesis testing within the networks context: Comparing two or more networks, e.g., we want to see if the density of two networks are equal. Prevalence of a motif/pattern, e.g., check whether the observed number of transitive triads is different from that expected as of by chance. Multivariate using ERGMs, e.g., jointly test whether homophily and two stars are the motifs that drive network structure. The latter we already review in the ERGM chapter. In this part, we will look at types one and two; both using non-parametric methods. "],["comparing-networks.html", "8.1 Comparing networks", " 8.1 Comparing networks Imagine that we have two graphs, \\((G_1,G_2) \\in \\mathcal{G}\\), and we would like to assess whether a given statistic \\(s(\\cdot)\\), e.g., density, is equal in both of them. Formally, we would like to asses whether \\(H_0: s(G_1) - s(G_2) = k\\) vs \\(H_a: s(G_1) - s(G_2) \\neq k\\). As usual, the true distribution of \\(s(\\cdot)\\) is unknown, thus, one approach that we could use is a non-parametric bootstrap test. 8.1.1 Network bootstrap The non parametric bootstrap and jackknife methods for social networks were introduced by (Snijders and Borgatti 1999). The method itself is used to generate standard errors for network level statistics. Both methods are implemented in the R package netdiffuseR. 8.1.2 When the statistic is normal When the we deal with things that are normally distributed, e.g., sample means like density11, we can make use of the Student’s distribution for making inference. In particular, we can use Bootstrap/Jackknife to approximate the standard errors of the statistic for each network: Since \\(s(G_i)\\sim \\mbox{N}(\\mu_i,\\sigma_i^2/m_i)\\) for \\(i\\in\\{1,2\\}\\), in the case of the density, \\(m_i = n_i * (n_i - 1)\\). The statistic is then: \\[ s(G_1) - s(G_0)\\sim \\mbox{N}(\\mu_1-\\mu_0, \\sigma_1^2/m_1 + \\sigma_1^2/m_2) \\] Thus \\[ \\frac{s(G_1) - s(G_0) - \\mu_1 + \\mu_2}{\\sqrt{\\sigma_1^2/{m_1} + \\sigma_1^2/{m_2}}} \\sim t_{m_1 + m_2 - 2} \\] But, if we are testing \\(H_0: \\mu_1 - \\mu_2 = k\\), then, under the null \\[ \\frac{s(G_1) - s(G_0) - k}{\\sqrt{\\sigma_1^2/{m_1} + \\sigma_1^2/{m_2}}} \\sim t_{m_1 + m_2 - 2} \\] Where We now proceede to approximate the variances. Using the plugin principle (Efron and Tibshirani 1994), we can approximate the variances using Bootstrap/Jackknife, i.e., compute \\(\\hat\\sigma_1^2\\approx\\sigma_1^2/m_1\\) and \\(\\hat\\sigma_2^2\\approx\\sigma_2^2/m_2\\). Using netdiffuseR library(netdiffuseR) # Obtain a 100 replicates sg1 &lt;- bootnet(g1, function(i, ...) sum(i)/(nnodes(i) * (nnodes(i) - 1)), R = 100) sg2 &lt;- bootnet(g2, function(i, ...) sum(i)/(nnodes(i) * (nnodes(i) - 1)), R = 100) # Retrieving the variances hat_sigma1 &lt;- sg1$var_t hat_sigma2 &lt;- sg2$var_t # And the actual values sg1 &lt;- sg1$t0 sg2 &lt;- sg2$t0 With the approximates in hand, we can then use the the “t-test table” to retrieve the corresponding value, in R: # Building the statistic k &lt;- 0 # For equal variances tstat &lt;- (sg1 - sg2 - k)/(sqrt(hat_sigma1 + hat_sigma2)) # Computing the pvalue m1 &lt;- nnodes(g1)*(nnodes(g1) - 1) m2 &lt;- nnodes(g2)*(nnodes(g2) - 1) pt(tstat, df = m1 + m2 - 2) 8.1.3 When the statistic is NOT normal In the case that the statistic is not normally distributed, we cannot use the t-statistic any longer. Nevertheless, the Bootstrap can come to help. While in general it is better to use distributions of pivot statistics (see (Efron and Tibshirani 1994)), we can still leverage the power of this method to make inferences. For this example, \\(s(\\cdot)\\) will be the range of the threshold in a diffusion graph. As before, imagine that we are dealing with an statistic \\(s(\\cdot)\\) for two different networks, and we would like to asses whether we can reject \\(H_0\\) or fail to reject it. The procedure is very similar: One approach that we can test is whether \\(k \\in \\mbox{ConfInt}(s(G_1) - s(G_2))\\). Building confidence intervals with bootstrap could be more intuitive. Like before, we use bootstrap to generate a distribution of \\(s(G_1)\\) and \\(s(G_2)\\), in R: # Obtain a 1000 replicates sg1 &lt;- bootnet(g1, function(i, ...) range(threshold(i)), R = 1000) sg2 &lt;- bootnet(g2, function(i, ...) range(threshold(i)), R = 1000) # Retrieving the distributions sg1 &lt;- sg1$boot$t sg2 &lt;- sg2$boot$t # Define the statistic sdiff &lt;- sg1 - sg2 Once we have sdiff, we can proceed and compute the, for example, 95% confidence interval, and evaluate whether \\(k\\) falls within. In R: diff_ci &lt;- quantile(sdiff, probs = c(0.025, .975)) This corresponds to what Efron and Tibshirani call “percentile interval.” This is easy to compute, but a better approach is using the “BCa” method, “Bias Corrected and Accelerated.” (TBD) References "],["examples.html", "8.2 Examples", " 8.2 Examples 8.2.1 Average of node-level stats Supposed that we would like to compare something like average indegree. In particular, for both networks, \\(G_1\\) and \\(G_2\\), we compute the average indegree per node: \\[ s(G_1) = \\mbox{AvgIndeg}(G_1) = \\frac{1}{n}\\sum_{i}\\sum_{j\\neq i}A^1_{ji} \\] where \\(A^1_{ji}\\) equals one if vertex \\(j\\) sends a tie to \\(i\\). In this case, since we are looking at an average, we have that \\(\\mbox{AvgIndeg}(G_1) \\sim N(\\mu_1, \\sigma^2_1/n)\\). Thus, taking advantage of the normality of the statistic, we can build a test statistic as follows: \\[ \\frac{s(G_1) - s(G_2) - k}{\\sqrt{\\hat\\sigma_{1}^2 + \\hat\\sigma_{2}^2}} \\sim t_{n_1 + n_2 - 2} \\] Where \\(\\hat\\sigma_i\\) is the bootstrap standard error, and \\(k = 0\\) when we are testing equality. This distributes \\(t\\) with \\(n_1+n_2-2\\) degrees of freedom. As a difference from the previous example using density, the degrees of freedom for this test are less as, instead of having an average across all entries of the adjacency matrix, we have an average across all vertices. "],["network-diffussion-with-netdiffuser.html", "Chapter 9 Network diffussion with netdiffuseR", " Chapter 9 Network diffussion with netdiffuseR This chapter mainly was based on the 2018 and 2019 tutorials of netdiffuseR at the Sunbelt conference. The source code of the tutorials, taught by Thomas W. Valente and George G. Vega Yon (author of this book), is available here. "],["network-diffusion-of-innovation.html", "9.1 Network diffusion of innovation", " 9.1 Network diffusion of innovation 9.1.1 Diffusion networks Explains how new ideas and practices (innovations) spread within and between communities. While a lot of factors have been shown to influence diffusion (Spatial, Economic, Cultural, Biological, etc.), Social Networks is a prominent one. There are many components in the diffusion network model including network exposures, thresholds, infectiousness, susceptibility, hazard rates, diffusion rates (bass model), clustering (Moran’s I), and so on. 9.1.2 Thresholds One of the canonical concepts is the network threshold. Network thresholds (Valente, 1995; 1996), \\(\\tau\\), are defined as the required proportion or number of neighbors that lead you to adopt a particular behavior (innovation), \\(a=1\\). In (very) general terms \\[ a_i = \\left\\{\\begin{array}{ll} 1 &amp;\\mbox{if } \\tau_i\\leq E_i \\\\ 0 &amp; \\mbox{Otherwise} \\end{array}\\right. \\qquad E_i \\equiv \\frac{\\sum_{j\\neq i}\\mathbf{X}_{ij}a_j}{\\sum_{j\\neq i}\\mathbf{X}_{ij}} \\] Where \\(E_i\\) is i’s exposure to the innovation and \\(\\mathbf{X}\\) is the adjacency matrix (the network). This can be generalized and extended to include covariates and other network weighting schemes (that’s what netdiffuseR is all about). "],["the-netdiffuser-r-package.html", "9.2 The netdiffuseR R package", " 9.2 The netdiffuseR R package 9.2.1 Overview netdiffuseR is an R package that: Is designed for Visualizing, Analyzing, and Simulating network diffusion data (in general). Depends on some pretty popular packages: RcppArmadillo: So it’s fast, Matrix: So it’s big, statnet and igraph: So it’s not from scratch Can handle big graphs, e.g., an adjacency matrix with more than 4 billion elements (PR for RcppArmadillo) Already on CRAN with ~6,000 downloads since its first version, Feb 2016, A lot of features to make it easy to read network (dynamic) data, making it a companion of other net packages. 9.2.2 Datasets netdiffuseR has the three classic Diffusion Network Datasets: medInnovationsDiffNet Doctors and the innovation of Tetracycline (1955). brfarmersDiffNet Brazilian farmers and the innovation of Hybrid Corn Seed (1966). kfamilyDiffNet Korean women and Family Planning methods (1973). brfarmersDiffNet ## Dynamic network of class -diffnet- ## Name : Brazilian Farmers ## Behavior : Adoption of Hybrid Corn Seeds ## # of nodes : 692 (1001, 1002, 1004, 1005, 1007, 1009, 1010, 1020, ...) ## # of time periods : 21 (1946 - 1966) ## Type : directed ## Final prevalence : 1.00 ## Static attributes : village, idold, age, liveout, visits, contact, coo... (146) ## Dynamic attributes : - medInnovationsDiffNet ## Dynamic network of class -diffnet- ## Name : Medical Innovation ## Behavior : Adoption of Tetracycline ## # of nodes : 125 (1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, ...) ## # of time periods : 18 (1 - 18) ## Type : directed ## Final prevalence : 1.00 ## Static attributes : city, detail, meet, coll, attend, proage, length, ... (58) ## Dynamic attributes : - kfamilyDiffNet ## Dynamic network of class -diffnet- ## Name : Korean Family Planning ## Behavior : Family Planning Methods ## # of nodes : 1047 (10002, 10003, 10005, 10007, 10010, 10011, 10012, 10014, ...) ## # of time periods : 11 (1 - 11) ## Type : directed ## Final prevalence : 1.00 ## Static attributes : village, recno1, studno1, area1, id1, nmage1, nmag... (430) ## Dynamic attributes : - 9.2.3 Visualization methods set.seed(12315) x &lt;- rdiffnet( 400, t = 6, rgraph.args = list(k=6, p=.3), seed.graph = &quot;small-world&quot;, seed.nodes = &quot;central&quot;, rewire = FALSE, threshold.dist = 1/4 ) plot(x) plot_diffnet(x) plot_diffnet2(x) plot_adopters(x) plot_threshold(x) plot_infectsuscep(x, K=2) ## Warning in plot_infectsuscep.list(graph$graph, graph$toa, t0, normalize, : When ## applying logscale some observations are missing. plot_hazard(x) 9.2.4 Problems Using the diffnet object in intro.rda, use the function plot_threshold specifying shapes and colors according to the variables ItrustMyFriends and Age. Do you see any pattern? "],["simulation-of-diffusion-processes.html", "9.3 Simulation of diffusion processes", " 9.3 Simulation of diffusion processes Before we start, a review of the concepts we will be using here Exposure: Proportion/number of neighbors that have adopted an innovation at each point in time. Threshold: The proportion/number of your neighbors who had adopted at or one time period before ego (the focal individual) adopted. Infectiousness: How much \\(i\\)’s adoption affects her alters. Susceptibility: How much \\(i\\)’s alters’ adoption affects her. Structural equivalence: How similar is \\(i\\) to \\(j\\) in terms of position in the network. 9.3.1 Simulating diffusion networks We will simulate a diffusion network with the following parameters: Will have 1,000 vertices, Will span 20 time periods, The initial adopters (seeds) will be selected at random, Seeds will be a 10% of the network, The graph (network) will be small-world, Will use the WS algorithm with \\(p=.2\\) (probability of rewiring). Threshold levels will be uniformly distributed between [0.3, 0.7] To generate this diffusion network, we can use the rdiffnet function included in the package: # Setting the seed for the RNG set.seed(1213) # Generating a random diffusion network net &lt;- rdiffnet( n = 1e3, # 1. t = 20, # 2. seed.nodes = &quot;random&quot;, # 3. seed.p.adopt = .1, # 4. seed.graph = &quot;small-world&quot;, # 5. rgraph.args = list(p=.2), # 6. threshold.dist = function(x) runif(1, .3, .7) # 7. ) The function rdiffnet generates random diffusion networks. Main features: Simulating random graph or using your own, Setting threshold levels per node, Network rewiring throughout the simulation, and Setting the seed nodes. The simulation algorithm is as follows: If required, a baseline graph is created, Set of initial adopters and threshold distribution are established, The set of t networks is created (if required), and Simulation starts at t=2, assigning adopters based on exposures and thresholds: For each \\(i \\in N\\), if its exposure at \\(t-1\\) is greater than its threshold, then adopts, otherwise, continue without change. next \\(i\\) 9.3.2 Rumor spreading library(netdiffuseR) set.seed(09) diffnet_rumor &lt;- rdiffnet( n = 5e2, t = 5, seed.graph = &quot;small-world&quot;, rgraph.args = list(k = 4, p = .3), seed.nodes = &quot;random&quot;, seed.p.adopt = .05, rewire = TRUE, threshold.dist = function(i) 1L, exposure.args = list(normalized = FALSE) ) summary(diffnet_rumor) ## Diffusion network summary statistics ## Name : A diffusion network ## Behavior : Random contagion ## ----------------------------------------------------------------------------- ## Period Adopters Cum Adopt. (%) Hazard Rate Density Moran&#39;s I (sd) ## -------- ---------- ---------------- ------------- --------- ---------------- ## 1 25 25 (0.05) - 0.01 -0.00 (0.00) ## 2 78 103 (0.21) 0.16 0.01 0.01 (0.00) *** ## 3 187 290 (0.58) 0.47 0.01 0.01 (0.00) *** ## 4 183 473 (0.95) 0.87 0.01 0.01 (0.00) *** ## 5 27 500 (1.00) 1.00 0.01 - ## ----------------------------------------------------------------------------- ## Left censoring : 0.05 (25) ## Right centoring : 0.00 (0) ## # of nodes : 500 ## ## Moran&#39;s I was computed on contemporaneous autocorrelation using 1/geodesic ## values. Significane levels *** &lt;= .01, ** &lt;= .05, * &lt;= .1. plot_diffnet(diffnet_rumor, slices = c(1, 3, 5)) # We want to use igraph to compute layout igdf &lt;- diffnet_to_igraph(diffnet_rumor, slices=c(1,2))[[1]] pos &lt;- igraph::layout_with_drl(igdf) plot_diffnet2(diffnet_rumor, vertex.size = dgr(diffnet_rumor)[,1], layout=pos) 9.3.3 Difussion set.seed(09) diffnet_complex &lt;- rdiffnet( seed.graph = diffnet_rumor$graph, seed.nodes = which(diffnet_rumor$toa == 1), rewire = FALSE, threshold.dist = function(i) rbeta(1, 3, 10), name = &quot;Diffusion&quot;, behavior = &quot;Some social behavior&quot; ) plot_adopters(diffnet_rumor, what = &quot;cumadopt&quot;, include.legend = FALSE) plot_adopters(diffnet_complex, bg=&quot;tomato&quot;, add=TRUE, what = &quot;cumadopt&quot;) legend(&quot;topleft&quot;, legend = c(&quot;Disease&quot;, &quot;Complex&quot;), col = c(&quot;lightblue&quot;, &quot;tomato&quot;), bty = &quot;n&quot;, pch=19) 9.3.4 Mentor Matching # Finding mentors mentors &lt;- mentor_matching(diffnet_rumor, 25, lead.ties.method = &quot;random&quot;) # Simulating diffusion with these mentors set.seed(09) diffnet_mentored &lt;- rdiffnet( seed.graph = diffnet_complex, seed.nodes = which(mentors$`1`$isleader), rewire = FALSE, threshold.dist = diffnet_complex[[&quot;real_threshold&quot;]], name = &quot;Diffusion using Mentors&quot; ) summary(diffnet_mentored) ## Diffusion network summary statistics ## Name : Diffusion using Mentors ## Behavior : Random contagion ## ----------------------------------------------------------------------------- ## Period Adopters Cum Adopt. (%) Hazard Rate Density Moran&#39;s I (sd) ## -------- ---------- ---------------- ------------- --------- ---------------- ## 1 25 25 (0.05) - 0.01 -0.00 (0.00) ## 2 92 117 (0.23) 0.19 0.01 0.01 (0.00) *** ## 3 152 269 (0.54) 0.40 0.01 0.01 (0.00) *** ## 4 150 419 (0.84) 0.65 0.01 0.01 (0.00) *** ## 5 73 492 (0.98) 0.90 0.01 -0.00 (0.00) ** ## ----------------------------------------------------------------------------- ## Left censoring : 0.05 (25) ## Right centoring : 0.02 (8) ## # of nodes : 500 ## ## Moran&#39;s I was computed on contemporaneous autocorrelation using 1/geodesic ## values. Significane levels *** &lt;= .01, ** &lt;= .05, * &lt;= .1. cumulative_adopt_count(diffnet_complex) ## 1 2 3 4 5 ## num 25.00 80.00 183.0000 338.0000000 470.0000000 ## prop 0.05 0.16 0.3660 0.6760000 0.9400000 ## rate 0.00 2.20 1.2875 0.8469945 0.3905325 cumulative_adopt_count(diffnet_mentored) ## 1 2 3 4 5 ## num 25.00 117.000 269.000000 419.0000000 492.0000000 ## prop 0.05 0.234 0.538000 0.8380000 0.9840000 ## rate 0.00 3.680 1.299145 0.5576208 0.1742243 9.3.5 Example by changing threshold # Simulating a scale-free homophilic network set.seed(1231) X &lt;- rep(c(1,1,1,1,1,0,0,0,0,0), 50) net &lt;- rgraph_ba(t = 499, m=4, eta = X) # Taking a look in igraph ig &lt;- igraph::graph_from_adjacency_matrix(net) plot(ig, vertex.color = c(&quot;azure&quot;, &quot;tomato&quot;)[X+1], vertex.label = NA, vertex.size = sqrt(dgr(net))) # Now, simulating a bunch of diffusion processes nsim &lt;- 500L ans_1and2 &lt;- vector(&quot;list&quot;, nsim) set.seed(223) for (i in 1:nsim) { # We just want the cum adopt count ans_1and2[[i]] &lt;- cumulative_adopt_count( rdiffnet( seed.graph = net, t = 10, threshold.dist = sample(1:2, 500L, TRUE), seed.nodes = &quot;random&quot;, seed.p.adopt = .10, exposure.args = list(outgoing = FALSE, normalized = FALSE), rewire = FALSE ) ) # Are we there yet? if (!(i %% 50)) message(&quot;Simulation &quot;, i,&quot; of &quot;, nsim, &quot; done.&quot;) } ## Simulation 50 of 500 done. ## Simulation 100 of 500 done. ## Simulation 150 of 500 done. ## Simulation 200 of 500 done. ## Simulation 250 of 500 done. ## Simulation 300 of 500 done. ## Simulation 350 of 500 done. ## Simulation 400 of 500 done. ## Simulation 450 of 500 done. ## Simulation 500 of 500 done. # Extracting prop ans_1and2 &lt;- do.call(rbind, lapply(ans_1and2, &quot;[&quot;, i=&quot;prop&quot;, j=)) ans_2and3 &lt;- vector(&quot;list&quot;, nsim) set.seed(223) for (i in 1:nsim) { # We just want the cum adopt count ans_2and3[[i]] &lt;- cumulative_adopt_count( rdiffnet( seed.graph = net, t = 10, threshold.dist = sample(2:3, 500L, TRUE), seed.nodes = &quot;random&quot;, seed.p.adopt = .10, exposure.args = list(outgoing = FALSE, normalized = FALSE), rewire = FALSE ) ) # Are we there yet? if (!(i %% 50)) message(&quot;Simulation &quot;, i,&quot; of &quot;, nsim, &quot; done.&quot;) } ## Simulation 50 of 500 done. ## Simulation 100 of 500 done. ## Simulation 150 of 500 done. ## Simulation 200 of 500 done. ## Simulation 250 of 500 done. ## Simulation 300 of 500 done. ## Simulation 350 of 500 done. ## Simulation 400 of 500 done. ## Simulation 450 of 500 done. ## Simulation 500 of 500 done. ans_2and3 &lt;- do.call(rbind, lapply(ans_2and3, &quot;[&quot;, i=&quot;prop&quot;, j=)) We can simplify by using the function rdiffnet_multiple. The following lines of code accomplish the same as the previous code avoiding the for-loop (from the user’s perspective). Besides of the usual parameters passed to rdiffnet, the rdiffnet_multiple function requires R (number of repetitions/simulations), and statistic (a function that returns the statistic of interest). Optionally, the user may choose to specify the number of clusters to run it in parallel (multiple CPUs): ans_1and3 &lt;- rdiffnet_multiple( # Num of sim R = nsim, # Statistic statistic = function(d) cumulative_adopt_count(d)[&quot;prop&quot;,], seed.graph = net, t = 10, threshold.dist = sample(1:3, 500, TRUE), seed.nodes = &quot;random&quot;, seed.p.adopt = .1, rewire = FALSE, exposure.args = list(outgoing=FALSE, normalized=FALSE), # Running on 4 cores ncpus = 4L ) boxplot(ans_1and2, col=&quot;ivory&quot;, xlab = &quot;Time&quot;, ylab = &quot;Threshold&quot;) boxplot(ans_2and3, col=&quot;tomato&quot;, add=TRUE) boxplot(t(ans_1and3), col = &quot;steelblue&quot;, add=TRUE) legend( &quot;topleft&quot;, fill = c(&quot;ivory&quot;, &quot;tomato&quot;, &quot;steelblue&quot;), legend = c(&quot;1/2&quot;, &quot;2/3&quot;, &quot;1/3&quot;), title = &quot;Threshold range&quot;, bty =&quot;n&quot; ) 9.3.6 Problems Given the following types of networks: Small-world, Scale-free, Bernoulli, what set of \\(n\\) initiators maximizes diffusion? "],["statistical-inference.html", "9.4 Statistical inference", " 9.4 Statistical inference 9.4.1 Moran’s I Moran’s I tests for spatial autocorrelation. netdiffuseR implements the test in moran, which is suited for sparse matrices. We can use Moran’s I as a first look to whether there is something happening: let that be influence or homophily. 9.4.2 Using geodesics One approach is to use the geodesic (shortest path length) matrix to account for indirect influence. In the case of sparse matrices, and furthermore, in the presence of structural holes it is more convenient to calculate the distance matrix taking this into account. netdiffuseR has a function to do so, the approx_geodesic function, which, using graph powers, computes the shortest path up to n steps. This could be faster (if you only care up to n steps) than igraph or sns: # Extracting the large adjacency matrix (stacked) dgc &lt;- diag_expand(medInnovationsDiffNet$graph) ig &lt;- igraph::graph_from_adjacency_matrix(dgc) mat &lt;- network::as.network(as.matrix(dgc)) # Measuring times times &lt;- microbenchmark::microbenchmark( netdiffuseR = netdiffuseR::approx_geodesic(dgc), igraph = igraph::distances(ig), sna = sna::geodist(mat), times = 50, unit=&quot;ms&quot; ) The summary.diffnet method already runs Moran’s for you. What happens under the hood is: # For each time point we compute the geodesic distances matrix W &lt;- approx_geodesic(medInnovationsDiffNet$graph[[1]]) # We get the element-wise inverse W@x &lt;- 1/W@x # And then compute moran moran(medInnovationsDiffNet$cumadopt[,1], W) ## $observed ## [1] 0.06624028 ## ## $expected ## [1] -0.008064516 ## ## $sd ## [1] 0.03265066 ## ## $p.value ## [1] 0.02286087 ## ## attr(,&quot;class&quot;) ## [1] &quot;diffnet_moran&quot; 9.4.3 Structural dependence and permutation tests A novel statistical method (work-in-progress) that allows conducting inference. Included in the package, tests whether a particular network statistic depends on network structure Suitable to be applied to network thresholds (you can’t use thresholds in regression-like models!) 9.4.4 Idea Let \\(\\mathcal{G} = (V,E)\\) be a graph, \\(\\gamma\\) a vertex attribute, and \\(\\beta = f(\\gamma,\\mathcal{G})\\), then \\[\\gamma \\perp \\mathcal{G} \\implies \\mathbb{E}\\left[\\beta(\\gamma,\\mathcal{G})|\\mathcal{G}\\right] = \\mathbb{E}\\left[\\beta(\\gamma,\\mathcal{G})\\right]\\] This is, if for example time of adoption is independent on the structure of the network, then the average threshold level will be independent from the network structure as well. Another way of looking at this is that the test will allow us to see how probable is to have this combination of network structure and network threshold (if it is uncommon then we say that the diffusion model is highly likely) 9.4.4.1 Example Not random TOA To use this test, __netdiffuseR__ has the `struct_test` function. It simulates networks with the same density, and computes a particular statistic every time, generating an EDF (Empirical Distribution Function) under the Null hypothesis (p-values). # Simulating network set.seed(1123) net &lt;- rdiffnet(n=500, t=10, seed.graph = &quot;small-world&quot;) # Running the test test &lt;- struct_test( graph = net, statistic = function(x) mean(threshold(x), na.rm = TRUE), R = 1e3, ncpus=4, parallel=&quot;multicore&quot; ) # See the output test ## ## Structure dependence test ## # Simulations : 1,000 ## # nodes : 500 ## # of time periods : 10 ## -------------------------------------------------------------------------------- ## H0: E[beta(Y,G)|G] - E[beta(Y,G)] = 0 (no structure dependency) ## observed expected p.val ## 0.5513 0.2504 0.0000 Now we shuffle times of adoption, so that is random # Resetting TOAs (now will be completely random) diffnet.toa(net) &lt;- sample(diffnet.toa(net), nnodes(net), TRUE) # Running the test test &lt;- struct_test( graph = net, statistic = function(x) mean(threshold(x), na.rm = TRUE), R = 1e3, ncpus=4, parallel=&quot;multicore&quot; ) # See the output test ## ## Structure dependence test ## # Simulations : 1,000 ## # nodes : 500 ## # of time periods : 10 ## -------------------------------------------------------------------------------- ## H0: E[beta(Y,G)|G] - E[beta(Y,G)] = 0 (no structure dependency) ## observed expected p.val ## 0.2714 0.2585 0.4020 9.4.5 Regression analysis In regression analysis, we want to see if exposure, once we control for other covariates had any effect on the adoption of a behavior. In general, the big problem here is when we have a latent variable that co-determines both network and behavior. Unless we can control for such variable, regression analysis will be generically biased. On the other hand, if you can claim that either such variable doesn’t exist or you actually can control for it, then we have two options: lagged exposure models or contemporaneous exposure models. We will focus on the former. 9.4.5.1 Lagged exposure models In this type of model, we usually have the following \\[ y_t = f(W_{t-1}, y_{t-1}, X_i) + \\varepsilon \\] Furthermore, in the case of adoption, we have \\[ y_{it} = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if}\\quad \\rho\\sum_{j\\neq i}\\frac{W_{ijt-1}y_{it-1}}{\\sum_{j\\neq i}W_{ijt-1}} + X_{it}\\beta &gt; 0\\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\] In netdiffuseR is as easy as doing the following: # fakedata set.seed(121) W &lt;- rgraph_ws(1e3, 8, .2) X &lt;- cbind(var1 = rnorm(1e3)) toa &lt;- sample(c(NA,1:5), 1e3, TRUE) dn &lt;- new_diffnet(W, toa=toa, vertex.static.attrs = X) ## Warning in new_diffnet(W, toa = toa, vertex.static.attrs = X): -graph- is static ## and will be recycled (see ?new_diffnet). # Computing exposure and adoption for regression dn[[&quot;cohesive_expo&quot;]] &lt;- cbind(NA, exposure(dn)[,-nslices(dn)]) dn[[&quot;adopt&quot;]] &lt;- dn$cumadopt # Generating the data and running the model dat &lt;- as.data.frame(dn) ans &lt;- glm(adopt ~ cohesive_expo + var1 + factor(per), data = dat, family = binomial(link=&quot;probit&quot;), subset = is.na(toa) | (per &lt;= toa)) summary(ans) ## ## Call: ## glm(formula = adopt ~ cohesive_expo + var1 + factor(per), family = binomial(link = &quot;probit&quot;), ## data = dat, subset = is.na(toa) | (per &lt;= toa)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.1754 -0.8462 -0.6645 1.2878 1.9523 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.92777 0.05840 -15.888 &lt; 2e-16 *** ## cohesive_expo 0.23839 0.17514 1.361 0.173452 ## var1 -0.04623 0.02704 -1.710 0.087334 . ## factor(per)3 0.29313 0.07715 3.799 0.000145 *** ## factor(per)4 0.33902 0.09897 3.425 0.000614 *** ## factor(per)5 0.59851 0.12193 4.909 9.18e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2745.1 on 2317 degrees of freedom ## Residual deviance: 2663.5 on 2312 degrees of freedom ## (1000 observations deleted due to missingness) ## AIC: 2675.5 ## ## Number of Fisher Scoring iterations: 4 Alternatively, we could have used the new function diffreg ans &lt;- diffreg(dn ~ exposure + var1 + factor(per), type = &quot;probit&quot;) summary(ans) ## ## Call: ## glm(formula = Adopt ~ exposure + var1 + factor(per), family = binomial(link = &quot;probit&quot;), ## data = dat, subset = ifelse(is.na(toa), TRUE, toa &gt;= per)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.1754 -0.8462 -0.6645 1.2878 1.9523 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.92777 0.05840 -15.888 &lt; 2e-16 *** ## exposure 0.23839 0.17514 1.361 0.173452 ## var1 -0.04623 0.02704 -1.710 0.087334 . ## factor(per)3 0.29313 0.07715 3.799 0.000145 *** ## factor(per)4 0.33902 0.09897 3.425 0.000614 *** ## factor(per)5 0.59851 0.12193 4.909 9.18e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2745.1 on 2317 degrees of freedom ## Residual deviance: 2663.5 on 2312 degrees of freedom ## (1000 observations deleted due to missingness) ## AIC: 2675.5 ## ## Number of Fisher Scoring iterations: 4 9.4.5.2 Contemporaneous exposure models Similar to the lagged exposure models, we usually have the following \\[ y_t = f(W_t, y_t, X_t) + \\varepsilon \\] Furthermore, in the case of adoption, we have \\[ y_{it} = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if}\\quad \\rho\\sum_{j\\neq i}\\frac{W_{ijt}y_{it}}{\\sum_{j\\neq i}W_{ijt}} + X_{it}\\beta &gt; 0\\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\] Unfortunately, since \\(y_t\\) is in both sides of the equation, this models cannot be fitted using a standard probit or logit regression. Two alternatives to solve this: Using Instrumental Variables Probit (ivprobit in both R and Stata) Use a Spatial Autoregressive (SAR) Probit (SpatialProbit and ProbitSpatial in R). We won’t cover these here. 9.4.6 Problems Using the dataset stats.rda: Compute Moran’s I as the function summary.diffnet does. For this you’ll need to use the function toa_mat (which calculates the cumulative adoption matrix), and approx_geodesic (which computes the geodesic matrix). (see ?summary.diffnet for more details). Read the data as diffnet object, and fit the following logit model \\(adopt = Exposure*\\gamma + Measure*\\beta + \\varepsilon\\). What happens if you exclude the time-fixed effects? "],["stochastic-actor-oriented-models.html", "Chapter 10 Stochastic Actor Oriented Models", " Chapter 10 Stochastic Actor Oriented Models Stochastic Actor Oriented Models (SOAM), also known as Siena models were introduced by CITATION NEEDED. As a difference from ERGMs, Siena models look at the data generating process from the individuals’ point of view. Based on McFadden’s ideas of probabilistic choice, the model is founded in the following equation \\[ U_i(x) - U_i(x&#39;) \\sim \\mbox{Extreame Value Distribution} \\] In other words, individuals choose between states \\(x\\) and \\(x&#39;\\) in a probabilistic way (with some noise), \\[ \\frac{\\mbox{exp}\\left\\{f_i^Z(\\beta^z,x, z)\\right\\}}{\\sum_{Z&#39;\\in\\mathcal{C}}\\mbox{exp}\\left\\{f_i^{Z}(\\beta, x, z&#39;)\\right\\}} \\] snijders_(sociological methodology 2001) (Snijders, Bunt, and Steglich 2010, @lazega2015, @Ripley2011) References "],["datasets-1.html", "A Datasets ", " A Datasets "],["sns-data.html", "A.1 SNS data", " A.1 SNS data A.1.1 About the data This data is part of the NIH Challenge grant # RC 1RC1AA019239 “Social Networks and Networking That Puts Adolescents at High Risk”. In general terms, the SNS’s goal was(is) “Understand the network effects on risk behaviors such as smoking initiation and substance use”. A.1.2 Variables The data has a wide structure, which means that there is one row per individual, and that dynamic attributes are represented as one column per time. photoid Photo id at the school level (can be repeated across schools). school School id. hispanic Indicator variable that equals 1 if the indivual ever reported himself as hispanic. female1, …, female4 Indicator variable that equals 1 if the individual reported to be female at the particular wave. grades1,…, grades4 Academic grades by wave. Values from 1 to 5, with 5 been the best. eversmk1, …, eversmk4 Indicator variable of ever smoking by wave. A one indicated that the individual had smoked at the time of the survey. everdrk1, …, everdrk4 Indicator variable of ever drinking by wave. A one indicated that the individual had drink at the time of the survey. home1, …, home4 Factor variable for home status by wave. A one indicates home ownership, a 2 rent, and a 3 a “I don’t know”. During the survey, participants were asked to name up to 19 of their school friends: sch_friend11, …, sch_friend119 School friends nominations (19 in total) for wave 1. The codes are mapped to the variable photoid. sch_friend21, …, sch_friend219 School friends nominations (19 in total) for wave 2. The codes are mapped to the variable photoid. sch_friend31, …, sch_friend319 School friends nominations (19 in total) for wave 3. The codes are mapped to the variable photoid. sch_friend41, …, sch_friend419 School friends nominations (19 in total) for wave 4. The codes are mapped to the variable photoid. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
